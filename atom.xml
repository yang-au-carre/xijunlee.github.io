<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Xijun&#39;s Homepage</title>
  <subtitle>Exploring with Curiosity!</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xijunlee.github.io/"/>
  <updated>2019-03-16T04:19:28.447Z</updated>
  <id>https://xijunlee.github.io/</id>
  
  <author>
    <name>Xijun (Ted) LI</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Reunderstanding The Mathematics Behind Principal Component Analysis</title>
    <link href="https://xijunlee.github.io/2019/03/10/reunderstanding-of-PCA/"/>
    <id>https://xijunlee.github.io/2019/03/10/reunderstanding-of-PCA/</id>
    <published>2019-03-10T06:54:13.000Z</published>
    <updated>2019-03-16T04:19:28.447Z</updated>
    
    <content type="html"><![CDATA[<p>As we all know, Principal Component Analysis (PCA) is a dimensionality reduction algorithm that can be used to significantly speed up your unsupervised feature learning algorithm. Most of us just know the procedure of PCA. In other words, we know how to use the algorithm but do not know how it comes. In this post, I summarize the procedure and mathematics of PCA based on materials of reference.</p>
<h2 id="Procedure-of-PCA"><a href="#Procedure-of-PCA" class="headerlink" title="Procedure of PCA"></a>Procedure of PCA</h2><p>Suppose we have a dataset  ${x^{(1)}, x^{(2)},…, x^{(m)} }$ with n dimension inputs. We want to reduce the data from $n$ dimension to $k$ dimension $(k&lt;&lt;n)$ using PCA. The procedure of PCA is shown below (Owing to the mathematical formual rendering problem, I use picture to display the procedure): </p>
<a id="more"></a>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/20190316Reunderstanding_PCA/Procedure%20of%20PCA.png">
<h2 id="Mathematics-behind-PCA"><a href="#Mathematics-behind-PCA" class="headerlink" title="Mathematics behind PCA"></a>Mathematics behind PCA</h2><p>In the previous section, I present the procedure of PCA.  What a simple algorithm it looks like. However, do we actually know why we calculate the eigenvector of covariance matrix, getting the basis of new subspace.</p>
<p>The motivation of PCA is remaining as much information of raw data as possible by projecting  raw data into lower subspace. In other words, we hope remain as much variance of raw data as possible in the new space.</p>
<p>In the formula (5), we see the projection of raw data. The length (variance) of the projection of $x$ onto $u$ is given by $x^T u$. I.e., if $x^(i)$ is a point in the dataset, then its projection onto $u$ is distance $x^T u$ from the origin. Hence to maximize the variance, we can formulate it as an optimization problem:<br>$$<br>\max \quad \frac{1}{m} \Sigma_{i=1}^{m} ({x^{(i)}}^Tu)^2 \tag{6}<br>$$</p>
<p>$$<br>s.t. \quad \quad \Arrowvert u \Arrowvert = 1 \tag{7}<br>$$</p>
<p>This is a maximization problem with constraint. It can be reformulated as:<br>$$<br>\max \quad u^T(\frac{1}{m}\Sigma_{i=1}^m x^{(i)} {x^{(i)}}^T )u = u^T\Sigma u \tag{8}<br>$$</p>
<p>$$<br>u^Tu=1 \tag{9}<br>$$</p>
<p>To get the (local) optimum of the constrained problem, we use the method of Lagrange multipliers:<br>$$<br>L(u,\lambda) = u^T \Sigma u -\lambda (u^Tu-1) \tag{10}<br>$$<br>Note that $\lambda$ is the Lagrange multipliers not the eigenvalue of covariance matrix. Now let us take the partial derivatives of formula (10) with respect to $u$ and $\lambda$. Then let the derivatives equal to zero:<br>$$<br>\frac{\partial L}{\partial u} = u^T\Sigma - \lambda u = 0 \tag{11}<br>$$</p>
<p>$$<br>\frac{\partial L}{\partial \lambda} = u^Tu-1 = 0 \tag{12}<br>$$</p>
<p>We can see that the formula (11) is equivalent to formula (3). Thus choosing the new basis of lower space is equivalent to getting the top-k eigenvector of covariance matrix of raw data.</p>
<p>See! The dimensionality reduction problem is intrinsically a constrained maximization problem. We use the method of Lagrangian multiplier to solve the problem.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://59.80.44.100/cs229.stanford.edu/notes/cs229-notes10.pdf" target="_blank" rel="external">http://59.80.44.100/cs229.stanford.edu/notes/cs229-notes10.pdf</a></p>
<p><a href="http://ufldl.stanford.edu/wiki/index.php/PCA" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/PCA</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;As we all know, Principal Component Analysis (PCA) is a dimensionality reduction algorithm that can be used to significantly speed up your unsupervised feature learning algorithm. Most of us just know the procedure of PCA. In other words, we know how to use the algorithm but do not know how it comes. In this post, I summarize the procedure and mathematics of PCA based on materials of reference.&lt;/p&gt;
&lt;h2 id=&quot;Procedure-of-PCA&quot;&gt;&lt;a href=&quot;#Procedure-of-PCA&quot; class=&quot;headerlink&quot; title=&quot;Procedure of PCA&quot;&gt;&lt;/a&gt;Procedure of PCA&lt;/h2&gt;&lt;p&gt;Suppose we have a dataset  ${x^{(1)}, x^{(2)},…, x^{(m)} }$ with n dimension inputs. We want to reduce the data from $n$ dimension to $k$ dimension $(k&amp;lt;&amp;lt;n)$ using PCA. The procedure of PCA is shown below (Owing to the mathematical formual rendering problem, I use picture to display the procedure): &lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="Optimization" scheme="https://xijunlee.github.io/tags/Optimization/"/>
    
  </entry>
  
  <entry>
    <title>Panorama of AutoML Methods</title>
    <link href="https://xijunlee.github.io/2019/01/14/panorama/"/>
    <id>https://xijunlee.github.io/2019/01/14/panorama/</id>
    <published>2019-01-14T13:59:27.000Z</published>
    <updated>2019-01-15T00:03:30.388Z</updated>
    
    <content type="html"><![CDATA[<p>Eventually, I finished the three AutoML reading notes. In addition to these notes, I also draw a panaroma of AutoML techniques discussed in the previous notes. It is hard to say that this panaroma has covered all AutoML techniques being researched now. So guys add ideas to this :)</p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Panoroma.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Eventually, I finished the three AutoML reading notes. In addition to these notes, I also draw a panaroma of AutoML techniques discussed 
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="AutoML" scheme="https://xijunlee.github.io/tags/AutoML/"/>
    
      <category term="Reading Note" scheme="https://xijunlee.github.io/tags/Reading-Note/"/>
    
  </entry>
  
  <entry>
    <title>AutoML Reading Note 3</title>
    <link href="https://xijunlee.github.io/2019/01/14/NAS/"/>
    <id>https://xijunlee.github.io/2019/01/14/NAS/</id>
    <published>2019-01-14T13:12:30.000Z</published>
    <updated>2019-01-14T14:19:14.349Z</updated>
    
    <content type="html"><![CDATA[<p>This post is about  <em>Neural Architecture Search</em> in AutoML, corresponding to Chapter 3 of the book <a href="https://www.automl.org/book/" target="_blank" rel="external">AutoML</a>. Note that all references can be found in the book.</p>
<h1 id="What-is-neural-architecture-search"><a href="#What-is-neural-architecture-search" class="headerlink" title="What is neural architecture search?"></a>What is neural architecture search?</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.001.jpeg" alt=""></p>
<a id="more"></a>
<h1 id="How-to-do-neural-architecture-search"><a href="#How-to-do-neural-architecture-search" class="headerlink" title="How to do neural architecture search?"></a>How to do neural architecture search?</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.002.jpeg" alt=""></p>
<p>Search space defines which architectures can be represented in principle. Incorporating prior knowledge about properties well-suited for a task can reduce the size of the search space and simplify the search. However, this also introduces a human bias, which may prevent finding novel architectural building blocks that go beyond the current human knowledge.</p>
<p>The search strategy  details how to explore the search space. It encompasses the classical exploration-exploitation trade-off since, on the one hand, it is desirable to find well-performing architectures quickly, while on the other hand, premature convergence to  a region of suboptimal architectures should be avoided.</p>
<p>The objective of NAS is typically to find architectures that achieve high predictive performance on unseen data. Performance Estimation refers to the process of estimating this performance: the simplest option is to perform a standard training and validation of the architecture on data, but this is unfortunately computationally expensive and limits the number of architectures that can be explored.</p>
<h2 id="Search-space"><a href="#Search-space" class="headerlink" title="Search space"></a>Search space</h2><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.003.jpeg" alt=""></p>
<p>The left figure presented chain-structured neural networks that is also the most traditional one. Thus, the search space of NAS must include this type of network. The search space of chain-structured neural networks is then parametrized by:(i) the (maximum) number of layers n (possibly unbounded); (ii) the type of operation every layer can execute , e.g., pooling, convolution, or more advanced layer types like depthwise separable convolutions [13] or dilated convolutions [67]; and (iii) hyperparameters associated with the operation, e.g., number of lters, kernel size and strides for a convolutional layer [4, 58, 10], or simply number of units for fully-connected networks [40].</p>
<p>Another recently emerging network type is cell (also called block) networks, motivated by hand-crafted architectures consisting of repeated motifs [61, 27, 28], Zoph et al. [74] and Zhong et al. [70] propose to search for such motifs, dubbed cells or blocks, respectively, rather than for whole architectures. Zoph et al. [74] optimize two different kind of cells: a normal cell that preservers the dimensionality of the input and a reduction cell which reduces the spatial dimension. The final architecture is then built by stacking these cells in a predefined manner.</p>
<h2 id="Search-strategy"><a href="#Search-strategy" class="headerlink" title="Search strategy"></a>Search strategy</h2><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.004.jpeg" alt=""></p>
<p>The search strategy falls into four main categories: randon search, evolutionary methods, reinforcement learning, and gradient based methods. I am most interested with the evolutionary methods and reinforcement learning. Thus I give more details of these two methods in the following.</p>
<h3 id="Reinforcement-learning"><a href="#Reinforcement-learning" class="headerlink" title="Reinforcement learning"></a>Reinforcement learning</h3><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.005.jpeg" alt=""></p>
<p>Google brain has published a paper in ICLR 2017, which present how they use reinforcement learning technique to learn a better neural architecture. This paper is based on such an observation that the structure and connectivity of a neural network can be typically specified by a variable-length string.</p>
<p>It is therefore possible to use a recurrent network – the controller – to generate such string. Training the network specified by the string – the “child network” – on the real data will result in an accuracy on a validation set. Using this accuracy as the reward signal, we can compute the policy gradient to update the controller. As a result, in the next iteration, the controller will give higher probabilities to architectures that receive high accuracies. In other words, the controller will learn to improve its search over time.</p>
<p>I summurize this paper as the following points:</p>
<blockquote>
<p><em>Flowchart of their algorithm</em>: In Neural Architecture Search, we use a controller to generate architectural hyperparameters of neural networks. To be flexible, the controller is implemented as a recurrent neural network. Let’s suppose we would like to predict feedforward neural networks with only convolutional layers, we can use the controller to generate their hyperparameters as a sequence of tokens. </p>
<p>In our experiments, the process of generating an architecture stops if the number of layers exceeds a certain value. This value follows a schedule where we increase it as training progresses. Once the controller RNN finishes generating an architecture, a neural network with this architecture is built and trained. At convergence, the accuracy of the network on a held-out validation set is recorded.</p>
<p><em>Parameter update</em>: The parameters of the controller RNN, $θ_c$, are then optimized in order to maximize the expected validation accuracy of the proposed architectures.</p>
<p><em>Parameter update using policy gradient descent</em> : The list of tokens that the controller predicts can be viewed as a list of actions $a[1:T]$ to design an architecture for a child network. At convergence, this child network will achieve an accuracy $R$ on a held-out dataset. We can use this accuracy $R$ as the reward signal and use reinforcement learning to train the controller. More concretely, to find the optimal architecture, we ask our controller to maximize its expected reward, represented by $J(θ_c)$: Since the reward signal $R$ is non-differentiable, we need to use a policy gradient method to iteratively update $θ_c$.</p>
</blockquote>
<h3 id="Neuro-evolutionary-methods"><a href="#Neuro-evolutionary-methods" class="headerlink" title="Neuro-evolutionary methods"></a>Neuro-evolutionary methods</h3><p>Evolutionary algorithms evolve a population of models, i.e., a set of (possibly trained) networks; in every evolution step, at least one model from the population is sampled and serves as a parent to generate offsprings by applying mutations to it. In the context of NAS, mutations are local operations, such as adding or removing a layer, altering the hyperparameters of a layer, adding skip connections, as well as altering training hyperparameters. After training the offsprings, their fitness (e.g., performance on a validation set) is evaluated and they are added to the population. Here I give two work of neuro-evolutionary methods. The first is from google where they utilize genetic algorithm to find complex classifier comparable to hand-designed models from very simple networks. The latter is from Huawei where they employ genetic algorithm to compress convolutionary networks from very complex network structures.</p>
<p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.006.jpeg" alt=""></p>
<p><em>“Large-scale evolution of image classifiers”</em>: The idea of google’s paper was to “sit back” and let evolution at scale do the work of constructing the architecture. Starting from very simple networks, the process found classifiers comparable to hand-designed models at the time.</p>
<p><em>“Towards Evolutional Compression”</em>: Compressing convolutional neural networks (CNNs) is essential for transferring the success of CNNs to a wide variety of applications to mobile devices. In contrast to directly recognizing subtle weights or filters as redundant in a given CNN, this paper presents an evolutionary method to automatically eliminate redundant convolution filters. We represent each compressed network as a binary individual of specific fitness. Then, the population is upgraded at each evolutionary iteration using genetic operations. As a result, an extremely compact CNN is generated using the fittest individual. In this approach, either large or small convolution filters can be redundant, and filters in the compressed network are more distinct. In addition, since the number of filters in each convolutional layer is reduced, the number of filter channels and the size of feature maps are also decreased, naturally improving both the compression and speed-up ratios. Experiments on benchmark deep CNN models suggest the superiority of the proposed algorithm over the state-of-the-art compression methods.</p>
<h2 id="Performance-estimiation-strategy"><a href="#Performance-estimiation-strategy" class="headerlink" title="Performance estimiation strategy"></a>Performance estimiation strategy</h2><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.007.jpeg" alt=""></p>
<p><em>Lower fidelities:</em> Such lower delities include shorter training times [74, 68], training on a subset of the data [33], on lower-resolution images [14], or with less filters per layer [74, 48]. While these low-fidelity approximations reduce the computational cost, they also introduce bias in the estimate as performance will typically be underestimated.</p>
<p><em>Learning curve extrapolation:</em> Domhan et al. [19] propose to extrapolate initial learning curves and terminate those predicted to perform poorly to speed up the architecture search process. Swersky et al. [60], Klein et al. [31], Baker et al. [5], Rawal and Miikkulainen [47] also consider architectural hyperparameters for predicting which partial learning curves are most promising.</p>
<p><em>Network morphisms:</em> Another approach to speed up performance estimation is to initialize the weights of novel architectures based on weights of other architectures that have been trained before. One way of achieving this, dubbed network morphisms [63], allows modifying an architecture while leaving the function represented by the network unchanged.</p>
<p><em>One-shot architecture search:</em> One-Shot Architecture Search is another promising approach for speeding up performance estimation, which treats all architectures as different subgraphs of a supergraph (the one-shot model) and shares weights between architectures that have edges of this supergraph in common [51, 9, 45, 38, 6 ]. Only the weights of a single one-shot model need to be trained (in one of various ways), and architectures (which are just subgraphs of the one-shot model) can then be evaluated without any separate training by inheriting trained weights from the one-shot model.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is about  &lt;em&gt;Neural Architecture Search&lt;/em&gt; in AutoML, corresponding to Chapter 3 of the book &lt;a href=&quot;https://www.automl.org/book/&quot;&gt;AutoML&lt;/a&gt;. Note that all references can be found in the book.&lt;/p&gt;
&lt;h1 id=&quot;What-is-neural-architecture-search&quot;&gt;&lt;a href=&quot;#What-is-neural-architecture-search&quot; class=&quot;headerlink&quot; title=&quot;What is neural architecture search?&quot;&gt;&lt;/a&gt;What is neural architecture search?&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML3/Intro%20To%20AutoML.001.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="AutoML" scheme="https://xijunlee.github.io/tags/AutoML/"/>
    
      <category term="Reading Note" scheme="https://xijunlee.github.io/tags/Reading-Note/"/>
    
      <category term="Meta Heuristic" scheme="https://xijunlee.github.io/tags/Meta-Heuristic/"/>
    
      <category term="Reinforcement Learning" scheme="https://xijunlee.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>A Solution to Mutual Reference in C++</title>
    <link href="https://xijunlee.github.io/2018/12/28/forwar-declaration/"/>
    <id>https://xijunlee.github.io/2018/12/28/forwar-declaration/</id>
    <published>2018-12-28T00:06:35.000Z</published>
    <updated>2018-12-28T13:33:06.397Z</updated>
    
    <content type="html"><![CDATA[<p>In the recent project, I was responsible for improving program efficiency. The whole project is written in Python. To this end, I’ve tried to rewrite C++ code to substitute Python code in some compute-intensive part. I’ve met the mutual reference when rewring the C++ code. The post shows the way I practice mutual reference in C++.</p>
<h2 id="Mutual-reference"><a href="#Mutual-reference" class="headerlink" title="Mutual reference"></a>Mutual reference</h2><p>There might often be such a situation that we have two classes, and they reference member variable or function of each other.  Just like Python code below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        b = B()</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">(self)</span>:</span></div><div class="line">        self.b.zap()</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">goo</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div><div class="line">        </div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></div><div class="line">        a = A()</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zot</span><span class="params">(self)</span>:</span></div><div class="line">        self.a.foo()</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">zap</span><span class="params">(self)</span>:</span></div><div class="line">        <span class="keyword">pass</span></div></pre></td></tr></table></figure>
<a id="more"></a>
<p>This is what we called <em>mutual reference</em>. It is not a big deal when you write mutual reference in python as python interpreter takes too much `compile’ work for us, so just enjoy writing Python. But you’ll fail the compile in C++ if you write code in a Python fashion that doesn’t care the order of declaration and definition of class.</p>
<h2 id="Forward-declaration-in-C"><a href="#Forward-declaration-in-C" class="headerlink" title="Forward declaration in C++"></a>Forward declaration in C++</h2><h3 id="What-is-forward-declaration"><a href="#What-is-forward-declaration" class="headerlink" title="What is forward declaration?"></a>What is forward declaration?</h3><p>The right way of mutual reference in C++ is <em>forward declaration</em> or <em>incomplete declaration</em>. The following is a definition of forward declaration:</p>
<blockquote>
<p>An incomplete declaration is the keyword class or struct followed by the name of a class or structure type. It tells the compiler that the named class or struct type exists, but doesn’t say anything at all about the member functions or variables of the class or struct; this omission means that it is a (seriously) incomplete declaration of the type. Usually the compiler will be supplied with the complete declaration later in the compilation, which is why an incomplete declaration is often called a forward declaration - it is an advance forward announcement of the existence of the type. </p>
</blockquote>
<p>As you can see, forward declaration should appear in header file. When will forward declaration work in a header file?</p>
<ol>
<li>If the class type X appears only as the type of a parameter or a return type in a function prototype. </li>
<li>If the class type X is referred to only by pointer (X*) or reference (X&amp;), even as a member variable of a class declared in A.h.</li>
<li>If you are using an opaque type X as a member variable of a class declared in A.h. </li>
</ol>
<h3 id="A-toy-example"><a href="#A-toy-example" class="headerlink" title="A toy example"></a>A toy example</h3><p>Forward declarations are essential for the following problem: Suppose we have two classes whose member functions make use of either parameters or member functions of the other class. </p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> A &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(B b)</span><span class="comment">// Ex. 1: a parameter of the other class type</span></span></div><div class="line">	&#123;</div><div class="line">		b.zap(); <span class="comment">// Ex. 2: call a member function of the other class</span></div><div class="line">	&#125;</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">goo</span><span class="params">()</span></span></div><div class="line">    &#123;<span class="comment">/* whatever */</span>&#125;</div><div class="line">&#125;;</div><div class="line"><span class="keyword">class</span> B &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">zot</span><span class="params">(A a)</span> <span class="comment">// Ex. 3: a parameter of the other class type</span></span></div><div class="line">	&#123;</div><div class="line"> 	    a.goo(); <span class="comment">// Ex. 4: call a member function of the other class</span></div><div class="line">	&#125;</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">zap</span><span class="params">()</span></span></div><div class="line">	&#123; <span class="comment">/* whatever */</span> &#125;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>
<p>The compiler will report error when it tries to compile this bit of code. The problem is that when it compiles the declaration of class A, it won’t be able to understand the line labeled Ex. 1 - it hasn’t seen a declaration of B yet. Obviously it would have a problem with Ex. 2, because it doesn’t know about function zap either. But if the compiler could somehow understand the class A declaration, it would then have no problem with class B, because it would already know about class A when it sees Ex. 3 and Ex. 4. However, since the compiler can’t compile the class A declaration, it will not be able to compile the class B declaration either. </p>
<h3 id="How-we-make-forward-declaration-to-fix-the-problem"><a href="#How-we-make-forward-declaration-to-fix-the-problem" class="headerlink" title="How we make forward declaration to fix the problem?"></a>How we make forward declaration to fix the problem?</h3><p>Here’s how we solve this problem. We take the function definitions out of the first class declarations, so that the compiler will see all of the second class before it has to compile the code for the first class functions. Here is how the example would look: This example now compiles successfully; here is the story: When the compiler sees line Ex. 1, which is now just a function prototype, it knows that B is the name of a class (from the forward declaration), and since we have only a function prototype here, we are no longer trying to call the still-unknown zap member function in B. The compiler simply records the foo prototype and continues. The compiler can then handle the class B declaration with no problem because it got all it needed from the class A declaration.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> B; <span class="comment">// forward incomplete declaration - class B will be fully declared later</span></div><div class="line"><span class="keyword">class</span> A &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(B b)</span></span>; <span class="comment">// Ex. 1:</span></div><div class="line">	<span class="comment">/* rest omitted to save space */</span></div><div class="line">&#125;;</div><div class="line"><span class="keyword">class</span> B &#123;</div><div class="line">    <span class="keyword">public</span>:</div><div class="line">	<span class="comment">/* rest omitted to save space */</span></div><div class="line">&#125;;</div><div class="line"><span class="comment">// the function definition later or in a separate .cpp file</span></div><div class="line"><span class="keyword">void</span> A::foo(B b) <span class="comment">// Ex. 1B: define A's foo function after the B declaration! !</span></div><div class="line">&#123;</div><div class="line">	b.zap(); <span class="comment">// Ex. 2: call a member function of the other class</span></div><div class="line">&#125;</div></pre></td></tr></table></figure>
<p>This example now compiles successfully; here is the story: When the compiler sees line Ex. 1, which is now just a function prototype, it knows that B is the name of a class (from the forward declaration), and since we have only a function prototype here, we are no longer trying to call the still-unknown zap member function in B. The compiler simply records the foo prototype and continues. The compiler can then handle the class B declaration with no problem because it got all it needed from the class A declaration. In line Ex. 1B, class A’s function foo is defined outside of the class A declaration, after the compiler has seen the complete class B declaration. Notice the scope operator :: that tells the compiler that this foo is the one prototyped in the class A declaration. The compiler can handle the previously problematic Line Ex. 2 because it is now knows about B’s zap member function.</p>
<p>Note: we often add  the header guard for header file. We should make the guard compatible with header file name, just like this:</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment">//classA.h</span></div><div class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CLASSA_H</span></div><div class="line"><span class="meta">#<span class="meta-keyword">define</span> CLASSA_H</span></div><div class="line"></div><div class="line"><span class="keyword">class</span> B; <span class="comment">// forward incomplete declaration - class B will be fully declared later</span></div><div class="line"><span class="keyword">class</span> A &#123;</div><div class="line"><span class="keyword">public</span>:</div><div class="line">	<span class="function"><span class="keyword">void</span> <span class="title">foo</span><span class="params">(B b)</span></span>; <span class="comment">// Ex. 1:</span></div><div class="line">	<span class="comment">/* rest omitted to save space */</span></div><div class="line">&#125;;</div><div class="line"> </div><div class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></div></pre></td></tr></table></figure>
<p>Thanks to so many answers in Stackoverflow and David Kieras’s note.</p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="http://jatinganhotra.com/blog/2012/11/25/forward-class-declaration-in-c-plus-plus/" target="_blank" rel="external">http://jatinganhotra.com/blog/2012/11/25/forward-class-declaration-in-c-plus-plus/</a><br><a href="http://umich.edu/~eecs381/handouts/IncompleteDeclarations.pdf" target="_blank" rel="external">http://umich.edu/~eecs381/handouts/IncompleteDeclarations.pdf</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;In the recent project, I was responsible for improving program efficiency. The whole project is written in Python. To this end, I’ve tried to rewrite C++ code to substitute Python code in some compute-intensive part. I’ve met the mutual reference when rewring the C++ code. The post shows the way I practice mutual reference in C++.&lt;/p&gt;
&lt;h2 id=&quot;Mutual-reference&quot;&gt;&lt;a href=&quot;#Mutual-reference&quot; class=&quot;headerlink&quot; title=&quot;Mutual reference&quot;&gt;&lt;/a&gt;Mutual reference&lt;/h2&gt;&lt;p&gt;There might often be such a situation that we have two classes, and they reference member variable or function of each other.  Just like Python code below:&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;15&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;16&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;17&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;18&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;A&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        b = B()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;foo&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.b.zap()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;goo&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;B&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        a = A()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;zot&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        self.a.foo()&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;    &lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;zap&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(self)&lt;/span&gt;:&lt;/span&gt;&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;        &lt;span class=&quot;keyword&quot;&gt;pass&lt;/span&gt;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="C++" scheme="https://xijunlee.github.io/tags/C/"/>
    
      <category term="Program Language" scheme="https://xijunlee.github.io/tags/Program-Language/"/>
    
      <category term="Mutual Reference" scheme="https://xijunlee.github.io/tags/Mutual-Reference/"/>
    
      <category term="Forward Declaration" scheme="https://xijunlee.github.io/tags/Forward-Declaration/"/>
    
  </entry>
  
  <entry>
    <title>AutoML Reading Note 2</title>
    <link href="https://xijunlee.github.io/2018/12/09/meta-learning/"/>
    <id>https://xijunlee.github.io/2018/12/09/meta-learning/</id>
    <published>2018-12-09T15:41:25.000Z</published>
    <updated>2019-01-13T15:40:33.642Z</updated>
    
    <content type="html"><![CDATA[<p>This post is about meta learning in AutoML, corresponding to Chapter 2 of the book <a href="https://www.automl.org/book/" target="_blank" rel="external">AutoML</a>. Note that all references can be found in the book.</p>
<h1 id="What-is-meta-learning"><a href="#What-is-meta-learning" class="headerlink" title="What is meta learning?"></a>What is meta learning?</h1><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.017.jpeg" alt=""></p>
<a id="more"></a>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.018.jpeg" alt=""></p>
<h1 id="Challenges-of-meta-learning"><a href="#Challenges-of-meta-learning" class="headerlink" title="Challenges of meta learning"></a>Challenges of meta learning</h1><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.019.jpeg" alt=""></p>
<h1 id="How-to-achieve-meta-learning"><a href="#How-to-achieve-meta-learning" class="headerlink" title="How to achieve meta learning?"></a>How to achieve meta learning?</h1><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.020.jpeg" alt=""></p>
<h2 id="Learning-from-model-evaluation"><a href="#Learning-from-model-evaluation" class="headerlink" title="Learning from model evaluation"></a>Learning from model evaluation</h2><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.021.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.022.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.023.jpeg" alt=""></p>
<ol>
<li>Relative landmarks: a first measure for task similarity considers the relative (pairwise) performance difference.</li>
<li>Surrogate models: a more flexible way to transfer information is to build surrogate model s_j(\theta<em>i) = P</em>{i,j} for all prior task, trained using all available P. One can the define task similarity in terms of the error between s_j(\theta<em>i) and P</em>{i,new}: if the surrogate model for t<em>j can generate accurate prediction for t</em>{new}, then those tasks are intrinsitically similar. This is usually done in combination with Bayesian optimization to determine the next \theta.</li>
<li>Warm-started multi-task learning: another approach to relate prior tasks tj is to learn a joint task representation using P. In [114], task-specic Bayesian linear regression [20] surrogate models sj(i) are trained and combined in a feedforward Neural Network NN(i) which learns a joint task representation that can accurately predict Pi;new. The surrogate models are pre-trained on OpenML meta-data to provide a warm-start for optimizing NN(i) in a multi-task learning setting. Earlier work on multitask learning [165] assumed that we already have a set of `similar’ source tasks tj . It transfers information between these tj and tnew by building a joint GP model for Bayesian optimization that learns and exploits the exact relationship between the tasks.</li>
<li>Multi-armed bandits: another approach to relate prior tasks tj is to learn a joint task representation using P. In [114], task-specic Bayesian linear regression [20] surrogate models sj(i) are trained and combined in a feedforward Neural Network NN(i) which learns a joint task representation that can accurately predict Pi;new. The surrogate models are pre-trained on OpenML meta-data to provide a warm-start for optimizing NN(i) in a multi-task learning setting. Earlier work on multitask learning [165] assumed that we already have a set of `similar’ source tasks tj . It transfers information between these tj and tnew by building a joint GP model for Bayesian optimization that learns and exploits the exact relationshipbetween the tasks.</li>
</ol>
<h2 id="Learning-from-task-properties"><a href="#Learning-from-task-properties" class="headerlink" title="Learning from task properties"></a>Learning from task properties</h2><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.024.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.025.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.026.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.027.jpeg" alt=""></p>
<p>Self-play reinforcement learning approach: AlphaD3M [38] uses a self-play reinforcement learning approach in which the current state is represented by the current pipeline, and actions include the addition, deletion, or replacement of pipeline components. A Monte Carlo Tree Search (MCTS) generates pipelines, which are evaluated to train a recurrent neural network (LSTM) that can predict pipeline performance, in turn producing the action probabilities for the MCTS in the next round. The state description also includes meta-features of the current task, allowing the neural network to learn across tasks.</p>
<h2 id="Learning-from-prior-models"><a href="#Learning-from-prior-models" class="headerlink" title="Learning from prior models"></a>Learning from prior models</h2><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.028.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.029.jpeg" alt=""></p>
<h3 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h3><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.030.jpeg" alt=""></p>
<p>In transfer learning, we take models trained on one or more source tasks, and use them as starting points for creating a model on a similar target task. This can be done by forcing the target model to be structurally or otherwise similar to the source models.</p>
<p>Neural networks are exceptionally suitable for transfer learning because both the structure and the model parameters of the source models can be used as a good initialization for the target model, yielding a pre-trained model which can be further fine-tuned using available training data on new task.</p>
<h3 id="Meta-learning-amp-few-shot-learning"><a href="#Meta-learning-amp-few-shot-learning" class="headerlink" title="Meta learning&amp; few-shot learning"></a>Meta learning&amp; few-shot learning</h3><p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.031.jpeg" alt=""></p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.032.jpeg" alt=""></p>
<p>This paper is based on the motivation that the parameter update of base learner resembles the updates for cell state in an LSTM. Thus they propose an LSTM-based meta-learner optimizer that is trained to optimize a learner neural network classifier. The meta-learner captures both short-term knowledge within a task and long-term knowledge common among all the tasks. By using an objective that directly captures an optimization algorithm’s ability to have good generalization performance given only a set number of updates, the meta learner model is trained to converge a learner classifier to good solution quickly on each task. Additionally, the formulation of the meta-learner model allows it to learn a task-common initialization for the learner classifier, which captures fundamental knowledge shared among all the tasks.</p>
<p><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.033.jpeg" alt=""></p>
<p>Model-Agnostic Meta-Learning (MAML) provides a good initialization of a model’s parameters to achieve an optimal fast learning on a new task with only a small number of gradient steps while avoiding overfitting that may happen when using a small dataset.</p>
<p>In the diagram above, θ is the model’s parameters and the bold black line is the meta-learning phase. When we have, for example, 3 different new tasks 1, 2 and 3, a gradient step is taken for each task (the gray lines). We can see that the parameters θ are close to all the 3 optimal parameters of task 1, 2, and 3 which makes θ the best parameters initialization that can quickly adapt to different new tasks. As a result, only a small change in the parameters θ will lead to an optimal minimization of the loss function of any task.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This post is about meta learning in AutoML, corresponding to Chapter 2 of the book &lt;a href=&quot;https://www.automl.org/book/&quot;&gt;AutoML&lt;/a&gt;. Note that all references can be found in the book.&lt;/p&gt;
&lt;h1 id=&quot;What-is-meta-learning&quot;&gt;&lt;a href=&quot;#What-is-meta-learning&quot; class=&quot;headerlink&quot; title=&quot;What is meta learning?&quot;&gt;&lt;/a&gt;What is meta learning?&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;http://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML2/Intro%20To%20AutoML.017.jpeg&quot; alt=&quot;&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="AutoML" scheme="https://xijunlee.github.io/tags/AutoML/"/>
    
      <category term="Optimization" scheme="https://xijunlee.github.io/tags/Optimization/"/>
    
      <category term="Reading Note" scheme="https://xijunlee.github.io/tags/Reading-Note/"/>
    
      <category term="Meta Learning" scheme="https://xijunlee.github.io/tags/Meta-Learning/"/>
    
  </entry>
  
  <entry>
    <title>AutoML Reading Note 1</title>
    <link href="https://xijunlee.github.io/2018/11/25/HPO/"/>
    <id>https://xijunlee.github.io/2018/11/25/HPO/</id>
    <published>2018-11-25T04:22:39.000Z</published>
    <updated>2019-01-14T14:11:24.918Z</updated>
    
    <content type="html"><![CDATA[<p>Although there are currently several developed machine learning suites,such as Keras, Pytorch, etc, facilitating the prevalence of machine learning techniques, data scientists or machine learning engineers still need to face the difficulty of hyperparameter choice of machine learning models. It has been empirically proved that for a given model, different choices of hyperparameter  also result in very different performances (accuracy, recall, etc). There is a trend of automating the hyperparameter selection for machine learning model, which is part of AutoML.</p>
<p>Recently I have been reading the ``ongoing’’ book,  <a href="https://www.automl.org/book/" target="_blank" rel="external">AutoML</a>, of which Chapter 1 introduces the existing methods to solve the hyperparameter optimization of machine learning model, and discusses several open problems as well as future research direction of this subfield. I wrote the post after reading this chapter. Note that all references can be found in the book.</p>
<a id="more"></a>
<h1 id="What-is-hyper-parameter"><a href="#What-is-hyper-parameter" class="headerlink" title="What is hyper parameter?"></a>What is hyper parameter?</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.004.jpeg" alt=""></p>
<p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.005.jpeg" alt=""></p>
<p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.006.jpeg" alt=""></p>
<p>Hyperparameter optimization has led to new state-of-the-art performances for important machine learning benchmarks in several studies. Besides, Automated HPO is clearly more reproducible than manual search. It facilitates fair comparisons since dierent methods can only be compared fairly if they all receive the same level of tuning for the problem at hand.</p>
<h1 id="Problem-statement-of-hyperparameter-optimization"><a href="#Problem-statement-of-hyperparameter-optimization" class="headerlink" title="Problem statement of hyperparameter optimization"></a>Problem statement of hyperparameter optimization</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.007.jpeg" alt=""></p>
<h1 id="Methods-to-optimize-hyperparameter"><a href="#Methods-to-optimize-hyperparameter" class="headerlink" title="Methods to optimize hyperparameter"></a>Methods to optimize hyperparameter</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.008.jpeg" alt=""></p>
<h2 id="Black-hyperparameter-optimization"><a href="#Black-hyperparameter-optimization" class="headerlink" title="Black hyperparameter optimization"></a>Black hyperparameter optimization</h2><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.009.jpeg" alt=""></p>
<p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.010.jpeg" alt=""></p>
<p>Bayesian optimization is actually an iterative algorithm with the probabilisitic surrogate model and acquisition function. In each iteration, the surrogate model is fitted to all observations of the target function made so far. Then, the acquisition function, which uses the predictive distribution of the probabilisitic model, determines the utility of different candidate points, trading off exploration and exploitation.</p>
<h1 id="Multi-fidelity-optimization"><a href="#Multi-fidelity-optimization" class="headerlink" title="Multi-fidelity optimization"></a>Multi-fidelity optimization</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.011.jpeg" alt=""></p>
<h1 id=""><a href="#" class="headerlink" title=""></a><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.012.jpeg" alt=""></h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.013.jpeg" alt=""></p>
<p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.014.jpeg" alt=""></p>
<p>Multi-task Bayesian optimization  uses a multi-task Gaussian process to model the performance of related tasks and to automatically learn the tasks’ correlation during the optimization process. This method can dynamically switch between cheaper, low-fidelity tasks and the expensive, high-fidelity target task based on a cost-aware information-theoretic acquisition function. In practice, the proposed method starts exploring the conguration space on the cheaper task and only switches to the more expensive conguration space in later parts of the optimization, approximately halving the time required for HPO.</p>
<h1 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.015.jpeg" alt=""></p>
<h1 id="In-the-future"><a href="#In-the-future" class="headerlink" title="In the future"></a>In the future</h1><p><img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/AutoML1/Intro%20To%20AutoML.016.jpeg" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Although there are currently several developed machine learning suites,such as Keras, Pytorch, etc, facilitating the prevalence of machine learning techniques, data scientists or machine learning engineers still need to face the difficulty of hyperparameter choice of machine learning models. It has been empirically proved that for a given model, different choices of hyperparameter  also result in very different performances (accuracy, recall, etc). There is a trend of automating the hyperparameter selection for machine learning model, which is part of AutoML.&lt;/p&gt;
&lt;p&gt;Recently I have been reading the ``ongoing’’ book,  &lt;a href=&quot;https://www.automl.org/book/&quot;&gt;AutoML&lt;/a&gt;, of which Chapter 1 introduces the existing methods to solve the hyperparameter optimization of machine learning model, and discusses several open problems as well as future research direction of this subfield. I wrote the post after reading this chapter. Note that all references can be found in the book.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="AutoML" scheme="https://xijunlee.github.io/tags/AutoML/"/>
    
      <category term="Optimization" scheme="https://xijunlee.github.io/tags/Optimization/"/>
    
      <category term="Reading Note" scheme="https://xijunlee.github.io/tags/Reading-Note/"/>
    
  </entry>
  
  <entry>
    <title>My Presentation at SIGKDD 2018</title>
    <link href="https://xijunlee.github.io/2018/09/21/KDD2018/"/>
    <id>https://xijunlee.github.io/2018/09/21/KDD2018/</id>
    <published>2018-09-21T12:41:07.000Z</published>
    <updated>2018-11-25T04:37:15.762Z</updated>
    
    <content type="html"><![CDATA[<img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/sigkdd2018/WechatIMG183.jpeg">
<p>It is my honor to take a presentaion in KDD 2018, which introduces my recent work at Noah’s Ark Lab of Huawei. What a great experience!</p>
]]></content>
    
    <summary type="html">
    
      &lt;img src=&quot;https://xijun-album.oss-cn-hangzhou.aliyuncs.com/sigkdd2018/WechatIMG183.jpeg&quot;&gt;
&lt;p&gt;It is my honor to take a presentaion in KDD 201
    
    </summary>
    
    
      <category term="Something Acadameic" scheme="https://xijunlee.github.io/tags/Something-Acadameic/"/>
    
  </entry>
  
  <entry>
    <title>A Paper Has Been Accepted by SIGKDD 2018</title>
    <link href="https://xijunlee.github.io/2018/08/18/SIGKDD2018/"/>
    <id>https://xijunlee.github.io/2018/08/18/SIGKDD2018/</id>
    <published>2018-08-17T18:32:59.000Z</published>
    <updated>2018-08-17T18:45:09.285Z</updated>
    
    <content type="html"><![CDATA[<img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/sigkdd2018/Screen%20Shot%202018-08-18%20at%2002.34.35.png">
<p>My work during internship at Noah’s Ark Lab of Huawei, A Data-Driven Three-Layer Algorithm for Split Delivery Vehicle Routing Problem with 3D Container Loading Constraint, has been recently accepted by Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.</p>
<p>The abstract of this work is as follows:</p>
<p>Split Delivery Vehicle Routing Problem with 3D Loading Constraints (3L-SDVRP) can be seen as the most important problem in large-scale manufacturing logistics. The goal is to devise a strategy consisting of three NP-hard planning components: vehicle routing, cargo splitting and container loading, which shall be jointly optimized for cost savings. The problem is an enhanced variant of the classical logistics problem 3L-CVRP, and its complexity leaps beyond current studies of solvability. Our solution employs a novel data-driven three-layer search algorithm (DTSA), which we designed to improve both the efficiency and effectiveness of traditional meta-heuristic approaches, through learning from data and from simulation.</p>
<p>A detailed experimental evaluation on real data shows our algorithm is versatile in solving this practical complex constrained multi-objective optimization problem, and our framework may be of general interest. DTSA performs much better than the state-of-the-art algorithms both in efficiency and optimization performance. Our algorithm has been deployed in the UAT (User Acceptance Test) environment; conservative estimates suggest that the full usage of our algorithm would save millions of dollars in logistics costs per year, besides savings due to automation and more efficient routing.</p>
<p>You are available to this work by visiting the <a href="http://www.kdd.org/kdd2018/accepted-papers/view/a-data-driven-three-layer-algorithm-for-split-delivery-vehicle-routing-prob" target="_blank" rel="external">link</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;img src=&quot;https://xijun-album.oss-cn-hangzhou.aliyuncs.com/sigkdd2018/Screen%20Shot%202018-08-18%20at%2002.34.35.png&quot;&gt;
&lt;p&gt;My work during int
    
    </summary>
    
    
      <category term="Academy" scheme="https://xijunlee.github.io/tags/Academy/"/>
    
      <category term="SIGKDD" scheme="https://xijunlee.github.io/tags/SIGKDD/"/>
    
  </entry>
  
  <entry>
    <title>A Paper Has Been Accepted by CIKM 2018</title>
    <link href="https://xijunlee.github.io/2018/08/18/CIKM2018/"/>
    <id>https://xijunlee.github.io/2018/08/18/CIKM2018/</id>
    <published>2018-08-17T18:14:32.000Z</published>
    <updated>2018-08-17T18:44:21.810Z</updated>
    
    <content type="html"><![CDATA[<img src="https://xijun-album.oss-cn-hangzhou.aliyuncs.com/cikm2018/Screen%20Shot%202018-08-18%20at%2002.33.49.png">
<p>My another work collaborated with Noah’s Ark Lab of Huawei, A Two-Layer Algorithmic Framework for Service Provider Configuration and Planning with Optimal Spatial Matching, has been recently accepted by ACM International Conference on Information and Knowledge Management (CIKM 2018).</p>
<p>The abstract of this work is as follows:</p>
<p>Industrial telecommunication applications prefer to run at the optimal capacity configuration to achieve the required Quality of Service (QoS) at the minimum cost. The optimal capacity configuration is usually achieved through the selection of cell towers capacities and locations. Given a set of service providers (e.g., cell towers) and a set of customers (e.g., major residential areas), where each customer has an amount of demand and each provider has multiple candidate capacities and corresponding costs, the optimal capacity selection is configured through spatial matching to satisfy the demand of each customer at the minimum cost. However, existing solutions developed for spatial matching, in which each provider’s capacity is fixed, cannot be directly applied to the capacity configuration problem with multiple capacities and location selections. In this paper, we are the first to study Service Provider Configuration and Planning with Optimal Spatial Matching (SPC-POSM) problem, in which the objectives are 1) to select the proper capacity for each provider at the minimum total cost and 2) to assign providers’ service to satisfy the demand of each customer on a condition that the matching distance is no more than service quality requirement. We prove that SPC-POSM problem is NP-hard and design an efficient two-layer meta-heuristic framework to solve the problem. Unsupervised learning technique is designed to accelerate the calculation and a novel local search mechanism is embedded to further improve solution quality. Extensive experimental results on both real and synthetic datasets verify the effectiveness and efficiency of the proposed framework.</p>
<p>The camery-ready version is going to be available in Oct. 2018.</p>
]]></content>
    
    <summary type="html">
    
      &lt;img src=&quot;https://xijun-album.oss-cn-hangzhou.aliyuncs.com/cikm2018/Screen%20Shot%202018-08-18%20at%2002.33.49.png&quot;&gt;
&lt;p&gt;My another work coll
    
    </summary>
    
    
      <category term="CIKM" scheme="https://xijunlee.github.io/tags/CIKM/"/>
    
      <category term="Academy" scheme="https://xijunlee.github.io/tags/Academy/"/>
    
  </entry>
  
  <entry>
    <title>Reunderstanding Evolutionary Algorithm Based on Gradient Descent Method</title>
    <link href="https://xijunlee.github.io/2018/06/05/Method/"/>
    <id>https://xijunlee.github.io/2018/06/05/Method/</id>
    <published>2018-06-05T15:53:55.000Z</published>
    <updated>2018-06-05T16:04:09.514Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Evolutionary-Algorithm"><a href="#Evolutionary-Algorithm" class="headerlink" title="Evolutionary Algorithm"></a>Evolutionary Algorithm</h2><p>To be written…</p>
<h2 id="Natural-Gradient-Descent"><a href="#Natural-Gradient-Descent" class="headerlink" title="Natural Gradient Descent"></a>Natural Gradient Descent</h2><p>The goal in standard backpropagation is to keep resampling the gradient of the network’s parameters after every update, and update them accordingly until reaching a (hopefully global) minimum.</p>
<p>However, there’s another way we can think of optimization. To better understand it, we first need to understand KL divergence.</p>
<a id="more"></a>
<p>In simple terms, KL divergence is a measure of how close a distribution is to another distribution. Here, it’s helpful to think of a neural network as a distribution over output values, given an input.</p>
<p>For example, let’s take a simple classification network that, given an input image, outputs probabilities that the image is either an apple, a banana, or an orange. If we had two of these networks with the same parameters, their KL divergence would be 0.</p>
<p>On the other hand, if the networks had different parameters, they would likely output different probabilities, given the same image.</p>
<p>The higher the difference between these probabilities are, the higher the KL divergence is between the two networks.</p>
<p>This brings us to the natural gradient. If we blindly update our network in the direction of its gradients, there are no guarantees the distribution of the new network will be similar to the old one.</p>
<p>To fix this, we first consider all combinations of parameters that result in a new network a constant KL divergence away from the old network. This constant value can be viewed as the step size or learning rate. Out of all these possible combinations, we choose the one that minimizes our loss function.</p>
<p>Basically, we’re adding in a constraint to our update, that the new network will behave relatively similar to our old one. Our step size corresponds directly to the actual distribution of the network, not it’s parameter’s values.</p>
<p>This comes with the added benefit of a more stable learning process. Especially when we’re updating using a randomly sampled batch, some outliers may make drastic changes to a network’s parameters. With natural gradient descent, a single update can’t make too much of an impact.</p>
<p>Furthermore, the natural gradient updates based on KL divergence, which only considers the output of a network. It doesn’t matter how the network is modeled. Replacing a sigmoid activation with a tanh function would change the standard gradient, but not the natural gradient.</p>
<p>Of course, in practice we don’t actually loop through every possible combination of parameters within a certain KL divergence. I won’t go into the mathematical explanations here, but it’s still true that the natural gradient is more computationally expensive compared to straight gradient descent.</p>
<p>For more in-depth details, check out <a href="https://arxiv.org/pdf/1301.3584v7.pdf" target="_blank" rel="external">this paper</a> or <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gradientDescent.pdf" target="_blank" rel="external">these notes</a>.</p>
<hr>
<p>Reference<br><a href="http://kvfrans.com/a-intuitive-explanation-of-natural-gradient-descent/" target="_blank" rel="external">A intuitive explanation of natural gradient descent</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Evolutionary-Algorithm&quot;&gt;&lt;a href=&quot;#Evolutionary-Algorithm&quot; class=&quot;headerlink&quot; title=&quot;Evolutionary Algorithm&quot;&gt;&lt;/a&gt;Evolutionary Algorithm&lt;/h2&gt;&lt;p&gt;To be written…&lt;/p&gt;
&lt;h2 id=&quot;Natural-Gradient-Descent&quot;&gt;&lt;a href=&quot;#Natural-Gradient-Descent&quot; class=&quot;headerlink&quot; title=&quot;Natural Gradient Descent&quot;&gt;&lt;/a&gt;Natural Gradient Descent&lt;/h2&gt;&lt;p&gt;The goal in standard backpropagation is to keep resampling the gradient of the network’s parameters after every update, and update them accordingly until reaching a (hopefully global) minimum.&lt;/p&gt;
&lt;p&gt;However, there’s another way we can think of optimization. To better understand it, we first need to understand KL divergence.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Evolutionary Algorithm" scheme="https://xijunlee.github.io/tags/Evolutionary-Algorithm/"/>
    
      <category term="Gradient Descent" scheme="https://xijunlee.github.io/tags/Gradient-Descent/"/>
    
      <category term="Probability Distribution" scheme="https://xijunlee.github.io/tags/Probability-Distribution/"/>
    
  </entry>
  
  <entry>
    <title>The Many Tribes of Artificial Intelligence [repost]</title>
    <link href="https://xijunlee.github.io/2018/03/14/tribe-of-ai/"/>
    <id>https://xijunlee.github.io/2018/03/14/tribe-of-ai/</id>
    <published>2018-03-14T13:51:12.000Z</published>
    <updated>2018-03-14T14:05:03.414Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h2><p>I found an interesting article discussing the different “tribe” in the field of AI, which could help us understand further the distinction and similarity of these tribes. Thus, I repo this article whose soure is <a href="https://medium.com/intuitionmachine/the-many-tribes-problem-of-artificial-intelligence-ai-1300faba5b60" target="_blank" rel="external">from here</a>.</p>
<h2 id="The-Article"><a href="#The-Article" class="headerlink" title="The Article"></a>The Article</h2><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/tribe_of_ai/fig1.jpg">
<a id="more"></a>
<p>One of the biggest confusions about “Artificial Intelligence” is that it is a very vague term. That’s because Artificial Intelligence or AI is a term that was coined way back in 1955 with extreme hubris:</p>
<p>We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire.<br>The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions, and concepts, solve kinds of problems now reserved for humans, and improve themselves. - Dartmouth AI Project Proposal; J. McCarthy et al.; Aug. 31, 1955.</p>
<p>AI is over half a century old and carries with it too much baggage. For a very long time, AI was dominated by Symbolists, that is rule-based systems that had “Zero Learning”. In the 1980’s a new kind of AI began to emerge, we termed this Machine Learning. Finally, we had at least “Simple Learning”. The big disruption, however, occurred this decade, when we stumbled upon “Deep Learning” and ever since it has been taking no prisoners.</p>
<p>This is, of course, a grossly simplified history of AI. There are actually many different approaches or tribes in AI. Pedro Domingo’s in his book, the Master Algorithm, talks about five different “tribes”. Not to be outdone, A YCombinator user “solidrocketfuel” posts about at least “21 different cultures”.<br>It is important for anyone that plans on doing AI to understand that there are differences in the approaches of the different tribes of AI. AI is not a homogenous field, but rather a field in constant tribal warfare. Here’s an overview:</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/tribe_of_ai/fig2.jpg">
<p>And a quick breakdown:</p>
<p><strong>Symbolists</strong> — Folks who used symbolic rule-based systems to make inferences. Most of AI has revolved around this approach. The approaches that used Lisp and Prolog are in this group, as well as the SemanticWeb, RDF, and OWL. One of the most ambitious attempts at this is Doug Lenat’s Cyc that he started back in the 80’s, where he has attempted to encode in logic rules all that we understand about this world. The major flaw is the brittleness of this approach, one always seems to find edge cases where one’s rigid knowledge base doesn’t seem to apply. Reality just seems to have this kind of fuzziness and uncertainty that is inescapable. It is like playing an endless game of Whack-a-mole.<br>Evolutionists- Folks who apply evolutionary processes like crossover and mutation to arrive at emergent intelligent behavior. This approach is typically known as Genetic Algorithms. We do see GA techniques used in replacement of a gradient descent approach in Deep Learning, so it’s not a approach that lives in isolation. Folks in this tribe also study cellular automata such as Conway’s Game of Life [CON] and Complex Adaptive Systems (CAS).</p>
<p><strong>Bayesians</strong> — Folks who use probabilistic rules and their dependencies to make inferences. Probabilistic Graph Models (PGMs) are a generalization of this approach and the primary computational mechanism is the Monte-Carlo method for sampling distributions. The approach has some similarity with the Symbolist approach in that there is a way to arrive at an explanation of the results. One other advantage of this approach is that there is a measure of uncertainty that can be expressed in the results. Edward is one library that mixes this approach with Deep Learning.<br>Kernel Conservatives — One of the most successful methods prior to the dominance of Deep Learning was SVM. Yann LeCun calls this glorified template matching. There is what is called a kernel trick that makes an otherwise non-linear separation problem into one that is linear. Practitioners in this field live in delight over the mathematical elegance of their approach. They believe the Deep Learners are nothing but alchemists conjuring up spells without the vaguest of understanding of the consequences.</p>
<p><strong>Tree Huggers</strong> — Folks who use tree-based models such as Random Forests and Gradient Boosted Decision Trees. These are essentially a tree of logic rules that slice up the domain recursively to build a classifier. This approach has actually been pretty effective in many Kaggle competitions. Microsoft has an approach that melds the tree based models with Deep Learning.</p>
<p><strong>Connectionists</strong> — Folks who believe that intelligent behavior arises from simple mechanisms that are highly interconnected. The first manifestation of this were Perceptrons back in 1959. This approach died and resurrected a few times since then. The latest incarnation is Deep Learning.<br>There are many sub-approaches under Deep Learning. This includes:<br>The Canadian Conspirators — Hinton, LeCun, Bengio et al. End-to-end deep learning without manual feature engineering.<br>Swiss Posse — Basically LSTM and that consciousness has been solved by two cooperating RNNs. This posse will have you lynched if you ever claim that you invented something before they did. GANs, the “coolest thing in the last 20 years” according to LeCun are also claimed to be invented by the posse.</p>
<p><strong>British AlphaGoist</strong> — Conjecture that AI = Deep Learning + Reinforcement Learning, despite LeCun’s claim that it is just the cherry on the cake. DeepMind is one of the major proponents in this area.</p>
<p><strong>Predictive Learners</strong> — I’m using the term Yann LeCun conjured up to describe unsupervised learning. The cake of AI or the dark matter of AI. This is a major unsolved area of AI. I, however, tend to believe that the solution is in “Meta-Learning”.<br>In addition to the above mainstream approaches, we also have:</p>
<p><strong>Compressionists</strong> — Cognition and learning are compression (Actually an idea that is shared by other tribes). The origins of Information theory derives from an argument about compression. This is a universal concept that it is more powerful than the all too often abused tool of aggregate statistics.</p>
<p><strong>Complexity Theorists</strong>- Employ methods coming from physics, energy-based models, complexity theory, chaos theory and statistical mechanics. Swarm AI likely fits into this category. If there’s any group that has a chance at coming up with a good explanation why Deep Learning works, then it is likely this group.</p>
<p><strong>Fuzzy Logicians</strong> — This approach was once quite popular, but for some reason, I haven’t heard much about it as of late. One would think that there would be a little more interest here considering the success of the also ‘fuzzy’ approach of Deep Learning. There was a recently published result that showed the use of Fuzzy rules defeating a fighter pilot in a mock dogfight.<br>Biological Inspirationalists — Folks who create models that are closer to what neurons appear in biology. Examples are the Numenta folks and the Spike-and-Integrate folks like IBM’s TrueNorth chip.</p>
<p><strong>Connectomeist</strong> — Folks who believe that the interconnection of the brain (i.e. Connectome) is where intelligence comes from. There’s a project that is trying to replicate a virtual worm and there is some ambitious heavily funded research [HCP] that is trying to map the brain in this way.<br>Information Integration Theorists — Argue that consciousness emerges from some internal imagination of machines that mirrors the causality of reality. The motivation of this group is that if we are ever to understand consciousness then we have to at least start thinking about it! I, however, can’t see the relationship of learning and consciousness in their approach. It is possible that they aren’t related at all! That’s maybe why we need sleep.<br><strong>PAC Theorists</strong> — Are folks that don’t really want to discuss Artificial Intelligence, rather prefer just studying intelligence because at least they know it exists! Their whole idea is that adaptive systems perform computation expediently such that they are all probably approximately correct. In short, intelligence does not have the luxury of massive computation.</p>
<p>In summary, there really is a bewildering array of alternative approaches to AI. I am certain that there are other approaches that I have missed. Some approaches are in opposition to each other, while other can be used together synergistically. However, what I want to point out is that a bit of an understanding of what is out there can help you navigate this space.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Preface&quot;&gt;&lt;a href=&quot;#Preface&quot; class=&quot;headerlink&quot; title=&quot;Preface&quot;&gt;&lt;/a&gt;Preface&lt;/h2&gt;&lt;p&gt;I found an interesting article discussing the different “tribe” in the field of AI, which could help us understand further the distinction and similarity of these tribes. Thus, I repo this article whose soure is &lt;a href=&quot;https://medium.com/intuitionmachine/the-many-tribes-problem-of-artificial-intelligence-ai-1300faba5b60&quot;&gt;from here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;The-Article&quot;&gt;&lt;a href=&quot;#The-Article&quot; class=&quot;headerlink&quot; title=&quot;The Article&quot;&gt;&lt;/a&gt;The Article&lt;/h2&gt;&lt;img src=&quot;http://xijun-album.oss-cn-hangzhou.aliyuncs.com/tribe_of_ai/fig1.jpg&quot;&gt;
    
    </summary>
    
    
      <category term="AI" scheme="https://xijunlee.github.io/tags/AI/"/>
    
      <category term="repost" scheme="https://xijunlee.github.io/tags/repost/"/>
    
  </entry>
  
  <entry>
    <title>2018: Beginning of Career</title>
    <link href="https://xijunlee.github.io/2018/02/23/2018-Career-Begins/"/>
    <id>https://xijunlee.github.io/2018/02/23/2018-Career-Begins/</id>
    <published>2018-02-23T09:13:22.000Z</published>
    <updated>2018-02-24T03:16:13.400Z</updated>
    
    <content type="html"><![CDATA[<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/musk%20starring.png">
<p>It is a new beginning of my life that I will start my career in 2018. May I keep curiosity and passion for exploring the future, just like Musk insisting on his rocket dream.</p>
]]></content>
    
    <summary type="html">
    
      &lt;img src=&quot;http://xijun-album.oss-cn-hangzhou.aliyuncs.com/musk%20starring.png&quot;&gt;
&lt;p&gt;It is a new beginning of my life that I will start my car
    
    </summary>
    
    
      <category term="Dairy" scheme="https://xijunlee.github.io/tags/Dairy/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGo Zero科普之从一无所知到也能吹水（下篇）</title>
    <link href="https://xijunlee.github.io/2017/11/05/Zero2/"/>
    <id>https://xijunlee.github.io/2017/11/05/Zero2/</id>
    <published>2017-11-05T14:42:12.000Z</published>
    <updated>2017-11-25T08:16:39.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>上篇介绍了Alpha Go的两个最主要的组件：卷积神经网络和蒙特卡洛树搜索。这篇开始进入正题，但在介绍AlphaGo Zero之前，我会先介绍其前身Alpha Go。因为这二者从本质上是一样的，只是前者在网络架构稍许不一样。我觉得先弄懂了Alpha Go之后，再看AlphaGo Zero的优化会有一种理所当然的感觉。</p>
<h2 id="Alpha-Go"><a href="#Alpha-Go" class="headerlink" title="Alpha Go"></a>Alpha Go</h2><p>这里开始进入正题了，有了对蒙特卡洛树搜索和卷积神经网络的初步认识，我觉得就比较容易理解Alpha Go的本质了。本节分为三个小节，分别是策略网络(Policy Network)，价值网络(Value Network)以及与前两个网络融合的蒙特卡洛树搜索。</p>
<a id="more"></a>
<h3 id="监督学习下的策略网络-Supervised-Learning-Policy-Network"><a href="#监督学习下的策略网络-Supervised-Learning-Policy-Network" class="headerlink" title="监督学习下的策略网络 Supervised Learning Policy Network"></a>监督学习下的策略网络 Supervised Learning Policy Network</h3><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG122.png">
<p>这里的监督学习策略网络p_sigma就是之前提到的学习人类棋手下棋风格的卷积神经网络。Deepmind的研究人员收集了大量的职业棋手的棋谱$(\vec{s},\vec{a})$后，利用随机梯度下降方法训练出了p_sigma。该网络的输入是Deepmind按照围棋规则和他们自己的理解构造的人工特征，详见下图，共有48个特征，每个特征是$19\times 19$的矩阵。因此该网络的输入$\vec{s}$是$19\times 19\times 48$的。然后输出是一个落子概率向量$\vec{a}$，即在每个合法位置落子的概率。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG123.png">
<p>虽然p_sigma能比较精确地模拟人类棋手的下期风格，但是因为网络结果比较复杂，网络每次计算时间比较长。为了能在后续的蒙特卡洛树搜索中能实现快速走子，Deepmind又训练了一个结构相对简单的快速走子网络(Rollout Network)p_pi。p_pi使用了更少的特征，结构相对p_sigma更简单，因此p_pi会算的更快但是算的没有p_sigma精确。</p>
<h3 id="强化学习下的策略网络-Reinforcement-Learning-Policy-Network"><a href="#强化学习下的策略网络-Reinforcement-Learning-Policy-Network" class="headerlink" title="强化学习下的策略网络 Reinforcement Learning Policy Network"></a>强化学习下的策略网络 Reinforcement Learning Policy Network</h3><p>先稍微简单介绍下强化学习。</p>
<blockquote>
<p>强化学习是一类算法, 是让计算机实现从一开始什么都不懂, 脑袋里没有一点想法, 通过不断地尝试, 从错误中学习, 最后找到规律, 学会了达到目的的方法. 这就是一个完整的强化学习过程. 实际中的强化学习例子有很多. 比如近期最有名的 Alpha go, 机器头一次在围棋场上战胜人类高手, 让计算机自己学着玩经典游戏 Atari, 这些都是让计算机在不断的尝试中更新自己的行为准则, 从而一步步学会如何下好围棋, 如何操控游戏得到高分. 既然要让计算机自己学, 那计算机通过什么来学习呢?原来计算机也需要一位虚拟的老师, 这个老师比较吝啬, 他不会告诉你如何移动, 如何做决定, 他为你做的事只有给你的行为打分, 那我们应该以什么形式学习这些现有的资源, 或者说怎么样只从分数中学习到我应该怎样做决定呢? 很简单, 我只需要记住那些高分, 低分对应的行为, 下次用同样的行为拿高分, 并避免低分的行为.比如老师会根据我的开心程度来打分, 我开心时, 可以得到高分, 我不开心时得到低分. 有了这些被打分的经验, 我就能判断为了拿到高分, 我应该选择一张开心的脸, 避免选到伤心的脸. 这也是强化学习的核心思想. 可以看出在强化学习中, 一种行为的分数是十分重要的. 所以强化学习具有分数导向性. 我们换一个角度来思考.这种分数导向性好比我们在监督学习中的正确标签.</p>
</blockquote>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG124.png">
<p>这样子大家应该对强化学习有一个感性的认知了。AlphaGo在构造好了监督学习下的策略网络后，就开始利用强化学习来提升策略网络的棋力了，记这个强化学习的策略网络为p_rho。p_rho在输入、输出和网络结构上与p_sigma可谓是完全一样。初始化时,p_rho的参数就等于p_sigma。之后该网络p_rho利用自我博弈和策略梯度强化学更新其参数，以提升其棋力。实验证明p_rho的棋力确实是要高于p_sigma的。这意味着强化学习网络青出于蓝而胜于蓝；这意味着强化学习网络从人类老师那里学到了围棋知识后，通过自我对弈学到了更多的围棋知识。</p>
<h3 id="价值网络-Value-Network"><a href="#价值网络-Value-Network" class="headerlink" title="价值网络 Value Network"></a>价值网络 Value Network</h3><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG125.png">
<p>Alpha Go中还构造了一个网络，即价值网络v。该网络的用途是给定一个棋局局面$\vec{s}$，立马根据$\vec{s}$输出我赢棋的概率$v$。</p>
<p>价值网络v的训练方法依旧是强化学习。更细一点说明如下：</p>
<p>1.针对棋局局面$\vec{s}$，不断利用强化策略网络p_rho进行自我对弈一直下棋到最后，就会得到一个输赢结果$z=+1$或者$z=-1$。其中1表示黑子赢棋，-1表示白子赢棋；<br>2.不断执行上一步骤，就能得到一系列的$(\vec{s},z)$对了。将这些$(\vec{s},z)$对作为训练数据来训练价值网络v。</p>
<h3 id="“融合”的蒙特卡洛树搜索"><a href="#“融合”的蒙特卡洛树搜索" class="headerlink" title="“融合”的蒙特卡洛树搜索"></a>“融合”的蒙特卡洛树搜索</h3><p>训练好了上述所有网络后，我们现在就已经万事具备了。上述的这些网络都是需要提前训练好的，这个训练过程的时间是非常漫长的，但是训练好之后就可以立马使用。不过，如果只单纯用策略网络去跟人下棋的话，一般是会被人类玩家虐的，因为这样的策略还是太简单了。所以，需要将上述网络与蒙特卡洛树搜索进行结合。下图正是Alpha Go中将策略网络和价值网络融合到蒙特卡洛树搜索的架构与算法流程：</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG126.png">
<p>首先，认识一下这个树结构。</p>
<p>树中的节点表示棋局中的一个局面$\vec{s}$，每个点的状态可以使用价值网络v进行评判其赢棋概率$v$。</p>
<p>每条边$(\vec{s},\vec{a})$表示基于某个局面$\vec{s}$的落子决策，每条边还存储三个状态$P(\vec{s},\vec{a})$，$N(\vec{s},\vec{a})$，$Q(\vec{s},\vec{a})$。$P(\vec{s},\vec{a})$是由策略网络p_sigma得到，表示基于当前状态$\vec{s}$下，采取策略$\vec{a}$的概率。$N(\vec{s},\vec{a})$表示经过边$(\vec{s},\vec{a})$的次数。$Q(\vec{s},\vec{a})$表示策略值，计算公式见上图。</p>
<p>Alpha Go Zero中的树搜索步骤可以总结为：</p>
<p>1.这个过程的第一步叫选择（Selection）。从根节点往下走，每次都选一个“最值得看的子节点”，直到来到一个“存在未扩展的子节点”的节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。选择的依据如上图公式所示。<br>2.第二步叫扩展（Expansion）。对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。我们利用策略网络p_sigma对其进行扩展，即基于当前局面下给出其所有可能落子位置的概率。<br>3.第三步是模拟（Simluation）。从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果$z_L$。另一方面，我们还可以利用价值网络v对当前这个局面进行评估，得到赢棋概率$v$。然后将这两个结果进行加权平均，得到一个更加客观的赢棋概率，记为$V$。<br>4.第四步是回溯（Backpropagation）。把模拟的结果更新回它的所有父节点上。这里需要更新的参数有$N(\vec{s},\vec{a})$，$Q(\vec{s},\vec{a})$。</p>
<p>好了，以上就是Alpha Go原理比较完整的概述了，如果大家对细节很感兴趣的话，还是阅读原文最好。</p>
<h2 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h2><h3 id="AlphaGo-Zero与Alpha-Go的不同之处"><a href="#AlphaGo-Zero与Alpha-Go的不同之处" class="headerlink" title="AlphaGo Zero与Alpha Go的不同之处"></a>AlphaGo Zero与Alpha Go的不同之处</h3><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG128.png">
<p>根据我的理解，Zero和之前的Alpha Go的主要差异有三点:</p>
<p>1.Zero采用了新的网络架构。将Alpha Go中的策略网络和价值网络融合成为一个网络，我认为这个想法很自然，因为策略网络和价值网络的输入层和网络隐层的结构完全一样，只不过前者输出落子概率向量，后者输出当前局面的赢棋概率。<br>2.Zero采用了目前最先进的卷积神经网络——深度残差网络(Deep Residual Network)，具体参见<a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">论文</a>。残差网络的出现一定程度地缓解了卷积神经网络中的梯度消失的问题，从而使得网络突破深度的瓶颈。换句话说，就是Zero采取的网络升级了，比Alpha Go有更强的学习能力。<br>3.Zero直接用棋盘状态的原始模样作为训练样本。Alpha Go使用了人工构造的特征作为训练样本输入网络，而Zero则是直接用棋盘状态的原始模样作为训练样本输入网络。</p>
<h3 id="AlphaGo-Zero中的强化学习"><a href="#AlphaGo-Zero中的强化学习" class="headerlink" title="AlphaGo Zero中的强化学习"></a>AlphaGo Zero中的强化学习</h3><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG127.png">
<p>Zero将Alpha Go中的策略网络和价值网络融合成为一个网络$f_\theta$。该网络的输入是棋盘当前状态以及前七步状态$\vec{s}$，输出是落子概率向量$\vec{a}$和赢棋概率$v$。</p>
<p>该网络的参数$\theta$是随机初始化的，也就是说没有学习人类知识，完全从零开始，这也是AlphaGo Zero中Zero取名的由来。而参数$\theta$的训练是利用在$f_\theta$指导下的蒙特卡洛树搜索的强化学习完成的。具体的步骤如下:</p>
<p>1.自我博弈(Self-play)。蒙特卡洛树搜索在$f_\theta$的指导下进行自我博弈，直到棋局结束。在这个自我博弈过程中，会得到一系列的棋盘状态$\vec{s}_i$，对应的落子概率向量$\vec{\pi}_i$，以及一个最后的赢棋结果$z$。</p>
<p>2.神经网络训练。利用第一步得到的一系列（棋盘状态，落子概率向量，赢棋结果）的来更新网络参数$\theta$。更新后的网络$f_\theta$又去指导蒙特卡洛树搜索进行自我博弈。</p>
<p>自我博弈和网络训练交替进行，直到网络参数$\theta$收敛为止。这样，我们就能得到一个无比强大的融合网络$f_\theta$。</p>
<h3 id="AlphaGo-Zero中的蒙特卡洛树搜索"><a href="#AlphaGo-Zero中的蒙特卡洛树搜索" class="headerlink" title="AlphaGo Zero中的蒙特卡洛树搜索"></a>AlphaGo Zero中的蒙特卡洛树搜索</h3><p>Zero中的树搜索和Alpha Go中的树搜索在流程和结构上可以说是一模一样，如下图所示，所以就不再赘述了。差异是Zero用了一个融合过后的网络$f_\theta$，这个网络干了Alpha Go中的策略网络和价值网络的活。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG130.png">
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>终于把Alpha Go与AlphaGo Zero系统地梳理了一遍了。不得不感慨Alpha Go以及AlphaGo Zero是科研与工程的极致产物，但也要认识到AlphaGo Zero并没有像媒体吹地那么神，其与真正的智能还是差的很远。最后，我就节选贴上田渊栋最近对于Zero的<a href="https://zhuanlan.zhihu.com/p/30750293?utm_source=wechat_session&amp;utm_medium=social" target="_blank" rel="external">评论</a>，其写得非常到位和客观。</p>
<blockquote>
<p>AlphaGo厉害的地方在于结合了工程和科研两方面的工作，通过大量计算资源和工程优化将一个方向推向了极致，并且同时借鉴了十年来大家在围棋上及在计算机视觉上的点滴进展，围棋和强化学习方向上像蒙特卡罗树搜索，自对弈（俗称“左右互搏”），随机走子盘面估值，用人工特征加浅层网络进行快速走子，权衡广度和深度搜索，权衡从头探索和先验知识，计算机视觉方向上像卷积神经网络（CNN），残差网络（ResNet），旋转翻折样本增强，等等。这些都不是DeepMind团队率先想出来的，而是过去的经验一点点积累起来得到的。只是过去的点滴进步并没有进入公众的视野，而AlphaGo达成了这最后的一步。AlphaGo在业内的影响力并不像在大众中（尤其是国内）那么大。在人工智能领域，每个方法都有优缺点和局限性，没有一种方法是万能的。让在AlphaGo上做出惊人结果的卷积神经网络，早先在计算机视觉上已获得巨大的成功，但让它去拟合含有大量离散特征的广告数据，大家都知道效果不佳；同样左右互搏对其它游戏也有相当不错的效果，但让它去做优化机器翻译，未必有人工标定的数据好。我想主要的问题在于我们之前把围棋这个千年的文化传承神化了，所以现在才有这样大的反弹；若是现在再把AlphaGo神化，那不免又掉进一模一样的圈子里去了。国外对于AlphaGo相比之下要理性很多，在各大论坛上也有更多技术上的深入探讨，而不是陷入玄之又玄的空谈。等到大家有这个实力AlphaGo拆解得七零八落，对它的每个部分都有比较清晰的了解的时候，那我相信它的神秘感，和现在这些疑惑和担扰，也就烟消云散。            — 田渊栋，Facebook人工智能组研究员</p>
</blockquote>
<hr>
<p>Reference<br><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" target="_blank" rel="external">https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf</a><br><a href="http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html" target="_blank" rel="external">http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html</a><br><a href="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" target="_blank" rel="external">https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/</a><br><a href="https://www.zhihu.com/question/39916945/answer/184152952" target="_blank" rel="external">https://www.zhihu.com/question/39916945/answer/184152952</a><br><a href="http://ieeexplore.ieee.org/document/6145622/" target="_blank" rel="external">http://ieeexplore.ieee.org/document/6145622/</a><br><a href="https://zhuanlan.zhihu.com/p/25345778" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25345778</a><br><a href="https://arxiv.org/abs/1412.6564" target="_blank" rel="external">https://arxiv.org/abs/1412.6564</a><br><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/</a><br><a href="https://arxiv.org/abs/1512.03385" target="_blank" rel="external">https://arxiv.org/abs/1512.03385</a><br><a href="https://zhuanlan.zhihu.com/p/30750293?utm_source=wechat_session&amp;utm_medium=social" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/30750293?utm_source=wechat_session&amp;utm_medium=social</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;上篇介绍了Alpha Go的两个最主要的组件：卷积神经网络和蒙特卡洛树搜索。这篇开始进入正题，但在介绍AlphaGo Zero之前，我会先介绍其前身Alpha Go。因为这二者从本质上是一样的，只是前者在网络架构稍许不一样。我觉得先弄懂了Alpha Go之后，再看AlphaGo Zero的优化会有一种理所当然的感觉。&lt;/p&gt;
&lt;h2 id=&quot;Alpha-Go&quot;&gt;&lt;a href=&quot;#Alpha-Go&quot; class=&quot;headerlink&quot; title=&quot;Alpha Go&quot;&gt;&lt;/a&gt;Alpha Go&lt;/h2&gt;&lt;p&gt;这里开始进入正题了，有了对蒙特卡洛树搜索和卷积神经网络的初步认识，我觉得就比较容易理解Alpha Go的本质了。本节分为三个小节，分别是策略网络(Policy Network)，价值网络(Value Network)以及与前两个网络融合的蒙特卡洛树搜索。&lt;/p&gt;
    
    </summary>
    
    
      <category term="CNN" scheme="https://xijunlee.github.io/tags/CNN/"/>
    
      <category term="Reinforcement Learning" scheme="https://xijunlee.github.io/tags/Reinforcement-Learning/"/>
    
      <category term="MCTS" scheme="https://xijunlee.github.io/tags/MCTS/"/>
    
  </entry>
  
  <entry>
    <title>AlphaGo Zero科普之从一无所知到也能吹水（上篇）</title>
    <link href="https://xijunlee.github.io/2017/11/03/Zero/"/>
    <id>https://xijunlee.github.io/2017/11/03/Zero/</id>
    <published>2017-11-03T15:58:38.000Z</published>
    <updated>2017-11-25T08:15:52.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>就在上个月的19号，AlphaGo Zero横空出世，号称是没有利用人类棋谱作为指导的AI打败了之前完虐李世石的AlphaGo Lee。这一消息着实轰动，19号这一天朋友圈被刷屏（但大部分还是非业内从业者在转发）。上个礼拜又在做调研，被老板抓去讲AlphaGo Zero。组会后，还是被老板认为讲解地不够直白，不能向更大的老板作报告（更大的老板可是没有AI的背景知识的）。我也慢慢意识到一个好的researcher不仅需要很强的理解能力，更需要极强的表达能力。为了锻炼自己的表达能力，这次博文就从非AI从业者的角度讲解这只有史以来最强的狗，希望同学们读完后也能跟别人吹吹Alpha Go了。</p>
<p>由于内容较多，该水文将分为上下两篇。上篇主要介绍Alpha Go的主要组件。如果你对卷积神经网络和蒙特卡洛树搜索很了解的话，可以跳过上篇。下篇进入正题，将详细介绍Alpha Go和Alpha Go Zero的原理。当然了，所有的讲解都是以非AI从业者的角度出发，不会有晦涩复杂的数学推导。</p>
<p>（主要的材料还是基于组会上的slides，仍旧是以图片的形式，十分抱歉</p>
<a id="more"></a>
<h2 id="Outline"><a href="#Outline" class="headerlink" title="Outline"></a>Outline</h2><p>下图是本文的一个提纲，大致的流程就是：首先是对问题的定义，这里将下围棋的活动定义成数学上的变换。然后分别介绍了AlphaGo的主要构件(building blocks)：蒙特卡洛树搜索(Monte Carlo Tree Search, MCTS)和卷积神经网络(Convolutional Neural Network, CNN)。可以说正是这二者的有效结合构造出了这个有史以来最强的围棋AI，其实想法相当简单，但是如果要弄清楚原理和优化细节还是水很深的。介绍完这两个主要的组件后，就进入了正题。但在介绍AlphaGo Zero之前，我会先介绍其前身Alpha Go。因为这二者从本质上是一样的，只是前者在网络架构稍许不一样。我觉得先弄懂了Alpha Go之后，再看AlphaGo Zero的优化会有一种理所当然的感觉。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG118.png">
<h2 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h2><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG117.png">
<p>首先，我们来将下围棋这么一个活动形式化。</p>
<p>围棋是在一张$19\times 19$的网格上面进行落子的活动，那么围棋的每一个状态都可以用一个$19\times 19$的矩阵$\textbf{X}$表示，矩阵中每一个元素表示对应位置的落子情况，1表示该位置被黑子占领，-1表示该位置被白子占领，0表示该位置是无子的。也可以将该矩阵展平成一个361维的向量$\vec{s}$。类似地，一个落子的决策也可以表示成一个361维的向量$\vec{a}$，因为面对每一个状态$\vec{s}$时总共会有361种落子选择（暂且先不考虑已落子或者其他不合法的落子选择，只是简单地认为有361种落子选择。</p>
<p>下围棋最终的目标是找到一系列的最优决策$\vec{a_1},\vec{a_2},…,\vec{a_n}$，使得我方占领棋盘上最大的地盘（其实我不会下围棋的，围棋规则还是论文调研时稍微看了下，如有不对，请指出）。</p>
<h2 id="蒙特卡洛树搜索-Monte-Carlo-Tree-Search"><a href="#蒙特卡洛树搜索-Monte-Carlo-Tree-Search" class="headerlink" title="蒙特卡洛树搜索 Monte Carlo Tree Search"></a>蒙特卡洛树搜索 Monte Carlo Tree Search</h2><p>对于现在的任何棋类活动，如何评判我当前下的这一步是不是臭棋，回想一下自己下棋的经历。我们是不是通常会在心里预演我下完这一步棋后，对方会怎么下，然后我再怎么下，然后对方再怎么下……厉害的或者有经验的棋手往往能在心里这么预演好几步，据报道柯洁能这么预演5步棋。设想一下，如果在你下棋时，有一个围棋上帝每次都能告诉你这一步下下去后你赢棋的概率有多大，那么你最后的赢棋概率是不是高很多呢？</p>
<p>在计算机科学中，这么个围棋上帝的角色可以通过树搜索(Tree Search)来做到。棋局状态及其落子决策在计算机算法中可以以树的形式表示出来，其中树的节点表示棋局的状态，边表示落子决策。理论上围棋的所有状态和落子决策是可以通过一棵树记录下来。而那个围棋上帝就是探索到这棵围棋树的最底部通过赢棋频率来判断当时落子决策的优劣，正如下面这个GIF所示：</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/mcts%20in%20go.gif">
<p>对，研究人员们通过这样的做法已经证明了部分棋类游戏是有必胜策略的，比如五子棋，只要先手就必胜。但是，不同棋类的搜索空间（即枚举所有的棋局状态）大小是不一样的，国际象棋的搜索空间是$35^{80}$种状态，这个看上去已经很大了是吧？围棋的搜索空间却达到了$250^{150}$种状态，这个数字已经比宇宙中所有粒子的总数目还要大得多。所以在Alpha Go出世之前，围棋被公认为是AI不可能挑战人类的项目。但是，之后就被打脸的。那么之前科学家是怎么做的五子棋和国际象棋等的AI的呢？正是蒙特卡洛搜索树，说白了，这也是一种树搜索，不过是高效剪枝后的树搜索算法，这对于棋类这种在线活动而言是很重要的，因为规则限定你必须在规定时间内做出决策。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG119.png">
<p>蒙特卡洛树的搜索步骤总共分为四步，大致如下。如果对其细节想有更多了解的同学请参考Reference中的<a href="http://ieeexplore.ieee.org/document/6145622/" target="_blank" rel="external">第五条</a>。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/mcts.jpg">
<p>上图中每个节点代表一个局面。而 A/B 代表这个节点被访问 B 次，黑棋胜利了 A 次。例如一开始的根节点是 12/21，代表总共模拟了 21 次，黑棋胜利了 12 次。</p>
<p>我们将不断重复一个过程（很多万次）：<br>1.这个过程的第一步叫选择（Selection）。从根节点往下走，每次都选一个“最值得看的子节点”，直到来到一个“存在未扩展的子节点”的节点，如图中的 3/3 节点。什么叫做“存在未扩展的子节点”，其实就是指这个局面存在未走过的后续着法。<br>2.第二步叫扩展（Expansion），我们给这个节点加上一个 0/0 子节点，对应之前所说的“未扩展的子节点”，就是还没有试过的一个着法。<br>3.第三步是模拟（Simluation）。从上面这个没有试过的着法开始，用快速走子策略（Rollout policy）走到底，得到一个胜负结果。按照普遍的观点，快速走子策略适合选择一个棋力很弱但走子很快的策略。因为如果这个策略走得慢（比如用 AlphaGo 的策略网络走棋），虽然棋力会更强，结果会更准确，但由于耗时多了，在单位时间内的模拟次数就少了，所以不一定会棋力更强，有可能会更弱。这也是为什么我们一般只模拟一次，因为如果模拟多次，虽然更准确，但更慢。<br>4.第四步是回溯（Backpropagation）。把模拟的结果加到它的所有父节点上。例如第三步模拟的结果是 0/1（代表黑棋失败），那么就把这个节点的所有父节点加上 0/1。</p>
<h2 id="卷积神经网络-Convolutional-Neural-Network"><a href="#卷积神经网络-Convolutional-Neural-Network" class="headerlink" title="卷积神经网络 Convolutional Neural Network"></a>卷积神经网络 Convolutional Neural Network</h2><p>卷积神经网络是目前深度学习中最炙手可热的大明星之一，其在图像分类、目标识别等多方面都有很好的应用。这里我们讲讲其在图像分类方面的应用，如下所示。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG120.png">
<p>该网络的任务是给它一张图，它能判断这张图是描述的什么东西。当然你先需要训练它，怎么训练呢？这个训练过程就很像小时候爸妈教我们看图识物一样。回想一下，那时候，父母是不是在我们面前摆一堆我们还不认识的东西的图片，然后反复告诉我们这个张图描述的是什么，那张图又描述的是什么。只不过现在这一教授过程的对象变成了计算机，既然要教计算机，那当然要采用计算机的语言和思维。</p>
<p>现在，假设我们要教计算机认识狗，猫，船以及鸟这四个对象。那么首先，我们要准备这四个对象的大量图片，然后告诉计算机这些图片分别属于哪一类。这两个步骤采用更加正式的描述如下：<br>1.首先你要准备好某个图像的像素矩阵，记为X，比如这张图片是船的。<br>2.生成一个对应的one-hot vector，长得像这样$(0,0,1,0)$，其中只有一个1，其余全为零。这个one-hot vector表示的意思是(狗，猫，船，鸟)=(0,0,1,0)，即告诉计算机这张图属于船的。将这样的one-hot vector记为Y。这个向量就是用来指导计算机认识图片的。<br>3.将上述的准备好的一系列X,Y“喂”给卷积神经网络，通过随机梯度下降等学习方法，该网络最终能学会不同对象的特征，从而在限定的物品类别中作出精确的分类。具体来说，就是经过学习后的网络，你随意丢一张图片给它，它能立马输出这张图属于(狗，猫，船，鸟)的相应概率。因为经过了学习，其中属于船的概率值是最高的，属于其他类别的概率接近于零。</p>
<p>卷积神经网络已经在很多数据集上被验证比普通的神经网络有更强大的特征抽取能力。所以目前只要涉及到图像的分类或者识别的任务，一般都想到用卷积神经网络来做。那么，下围棋这里怎么用到卷积神经网络呢？首先，我们要认识到下棋其实也是一个分类问题，就是当前给我一个棋局局面$\vec{s}$时，我如何在众多落子位置中选择一个最终要下的落子决策呢？我只需要选择我认为赢棋概率最高的位置下下去就行。这么看来下棋也就是一个多分类问题。既然卷积神经网络能模拟人类识别图像的能力，那么它是不是能用来学习人类棋手下棋的风格呢？对，早就有人这么干过了，Aja Huang在其<a href="https://arxiv.org/abs/1412.6564" target="_blank" rel="external">论文</a>中就实现过了这一过程，他从KGS围棋对战平台中获得了大量的对弈棋谱$\vec{s}$和在该状态$\vec{s}$ 人类棋手所做的落子决策$\vec{a}$，将这些$\vec{s}$和$\vec{a}$扔给一个卷积神经网络去学习。经过学习后，该网络就能模拟人类的风格进行下棋。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/alphagozero/WechatIMG121.png">
<p>那么很容易想到就是该神经网络的棋力应当是与它模仿的人类选手的棋力相当的。如果它模仿的人是一个臭棋手，那么它的下棋水平也会很臭，如果模仿的是高手，其水平也会相应提高。在Aja Huang这篇论文中，他们训练的卷积神经网络的棋力大概是业余六段。</p>
<p>从函数的角度解释卷积神经网络。不论是人还是机器，我们由某个状态$\vec{s}$想到对应的决策$\vec{a}$，都可以看成是一个函数映射$f$，即$f:\vec{s} \mapsto \vec{a}$。这个映射完全是由函数的参数$\Theta$决定，其实不管是人类还是机器，学习的过程就是在学这个参数$\Theta$。人类的认知经验其实就是抽象的$\Theta$，被存储在我们的大脑中。而神经网络通过模仿学习人类的经验参数$\Theta$，但这个学习过程存在误差（想想上课时老师教授你的东西，你也不可能全部听进去）。因此神经网络学到的参数只是趋近于$\Theta$的，记为$\theta$。然后$\theta$就被保存在非易失性存储器里，供下一次神经网络被调用时使用。</p>
<p>注意到，这样一个学习人类风格下棋的神经网络被称为策略网络$p$(Policy Network），在下文将会被反复提及到。</p>
<h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><p>本篇比较详细地介绍了Alpha Go中的两个主要组件：卷积神经网络和蒙特卡洛树搜索。正是这二者的有效结合构造出了有史以来最强的围棋AI。下篇将开始进入正题，详细解剖Alpha Go和AlphaGo Zero的架构和原理。</p>
<hr>
<p>Reference<br><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf" target="_blank" rel="external">https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf</a><br><a href="http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html" target="_blank" rel="external">http://www.nature.com/nature/journal/v550/n7676/full/nature24270.html</a><br><a href="https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/" target="_blank" rel="external">https://charlesliuyx.github.io/2017/10/18/深入浅出看懂AlphaGo元/</a><br><a href="https://www.zhihu.com/question/39916945/answer/184152952" target="_blank" rel="external">https://www.zhihu.com/question/39916945/answer/184152952</a><br><a href="http://ieeexplore.ieee.org/document/6145622/" target="_blank" rel="external">http://ieeexplore.ieee.org/document/6145622/</a><br><a href="https://zhuanlan.zhihu.com/p/25345778" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/25345778</a><br><a href="https://arxiv.org/abs/1412.6564" target="_blank" rel="external">https://arxiv.org/abs/1412.6564</a><br><a href="https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/" target="_blank" rel="external">https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/1-1-A-RL/</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;就在上个月的19号，AlphaGo Zero横空出世，号称是没有利用人类棋谱作为指导的AI打败了之前完虐李世石的AlphaGo Lee。这一消息着实轰动，19号这一天朋友圈被刷屏（但大部分还是非业内从业者在转发）。上个礼拜又在做调研，被老板抓去讲AlphaGo Zero。组会后，还是被老板认为讲解地不够直白，不能向更大的老板作报告（更大的老板可是没有AI的背景知识的）。我也慢慢意识到一个好的researcher不仅需要很强的理解能力，更需要极强的表达能力。为了锻炼自己的表达能力，这次博文就从非AI从业者的角度讲解这只有史以来最强的狗，希望同学们读完后也能跟别人吹吹Alpha Go了。&lt;/p&gt;
&lt;p&gt;由于内容较多，该水文将分为上下两篇。上篇主要介绍Alpha Go的主要组件。如果你对卷积神经网络和蒙特卡洛树搜索很了解的话，可以跳过上篇。下篇进入正题，将详细介绍Alpha Go和Alpha Go Zero的原理。当然了，所有的讲解都是以非AI从业者的角度出发，不会有晦涩复杂的数学推导。&lt;/p&gt;
&lt;p&gt;（主要的材料还是基于组会上的slides，仍旧是以图片的形式，十分抱歉&lt;/p&gt;
    
    </summary>
    
    
      <category term="CNN" scheme="https://xijunlee.github.io/tags/CNN/"/>
    
      <category term="Reinforcement Learning" scheme="https://xijunlee.github.io/tags/Reinforcement-Learning/"/>
    
      <category term="MCTS" scheme="https://xijunlee.github.io/tags/MCTS/"/>
    
  </entry>
  
  <entry>
    <title>从单纯型法到列生成算法</title>
    <link href="https://xijunlee.github.io/2017/10/12/%E4%BB%8E%E5%8D%95%E7%BA%AF%E5%9E%8B%E6%B3%95%E5%88%B0%E5%88%97%E7%94%9F%E6%88%90%E7%AE%97%E6%B3%95/"/>
    <id>https://xijunlee.github.io/2017/10/12/从单纯型法到列生成算法/</id>
    <published>2017-10-12T06:49:05.000Z</published>
    <updated>2017-11-06T13:04:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近在做混合整数规划相关的调研，主要是关于列生成算法 (Column Generation Algorithm)，搜了一圈，发现网路上并有很多相关的中文资料，难道这个算法太难了？还是普及度不够高？后来找到了一份相关的不错的英文讲义，从头看下来，也就知道了列生成算法大概是在干什么，但是其中原因还是不得要领。</p>
<p>之后，我又认真复习了单纯形法，才明白了列生成算法的初衷和要领，希望本文能把它讲明白。</p>
<p>（最近一直用图片的形式记录，是因为这些总结都是组会上的slides😅</p>
<a id="more"></a>
<h2 id="单纯型法-Simplex-Method"><a href="#单纯型法-Simplex-Method" class="headerlink" title="单纯型法 Simplex Method"></a>单纯型法 Simplex Method</h2><p>单纯形法是求解线性规划代表性的算法。其基于这么样的一个事实：线性规划是在凸集上的凸优化问题，因此其局部最优解也是全局最优解。而这些局部最优解是会出现在该凸集的顶点上，所以单纯形法的思路就是在这些顶点上移动，一直移动到局部最优的顶点。这些顶点也叫做基本可行解，那么这些移动也就是从一个基本可行解通过基变换(即从众多非基变量中选择一个进基，再从基变量中选择一个使其出基)变成另一个基本可行解，直到不能再使得优化目标更优为止（这里，我们采用线性规划的标准形式，即最小化目标函数）。这个基变换的操作涉及到进基和出基，那么如何选择哪个非基变量进基呢？这里就会涉及到reduced cost rate，即下图中的$c^r_j$。用单纯型法的语言来说就是，通过数次基变换，使得再也找不到一个非基变量使得reduced cost rate小于零，即再也找不到一个非基变量进基使得目标函数减小，这样我们就找到目标函数在其polyhedron上的最优值。单纯形法简单过程如下面这张图所示，但完整的推导请参考<a href="http://personal.vu.nl/l.stougie/Courses/ALP/BTonlyCh12345.pdf" target="_blank" rel="external">Introduction to Linear Optimization</a>的第二、三章相关内容。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/pic10.png">
<h2 id="列生成算法-Column-Generation-Algorithm"><a href="#列生成算法-Column-Generation-Algorithm" class="headerlink" title="列生成算法 Column Generation Algorithm"></a>列生成算法 Column Generation Algorithm</h2><p>单纯型法虽然能保证在数次迭代后找到最优解，但是其面对变量很多的线性规划问题就显得很弱了。因为它需要去在众多变量里进行基变换，这种枚举的工作量是可怕的。因此，有人基于单纯型法提出了列生成算法，其思路大概就是先把原问题(master problem)restrict到一个规模更小（即变量数比原问题少的）的restricted master problem，在restricted master problem上用单纯型法求最优解，但是此时求得的最优解只是restricted master problem上的，并不是master problem的最优解。此时，就需要通过一个subproblem去check在那些未被考虑的变量中是否有使得reduced cost rate小于零的呢（其具体的做法就是通过求解一个线性最大化问题，即求未被考虑的变量中的reduced cost rate的最大值）？如果有，那么我就把这个变量的相关系数列加入到restricted master problem的系数矩阵中。经过这样反复的迭代，直到subproblem中的reduced cost rate大于等于零，那么master problem就求到了最优解。更详细的过程参看以下两张图：</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/pic11.png">
<p>注意到，这里为什么要把restricted master problem转化到其对偶问题呢？我的理解是，虽然通过单纯型法直接求解restricted master problem能得到subproblem中所需的变量$u=c_BA_B^{-1}$，但是restricted master problem也可能是一个变量很多的线性规划。前面也说过了，单纯型法对变量很多的问题是无能为力的。因此通过单纯型法求restricted master problemde的对偶问题（将restricted master problem对偶一下，就能使得变量数大幅减小，因为这些变量转换成了对偶问题中的限制条件了），能更快地得到子问题想要的$u=c_BA_B^{-1}$。</p>
<p>再次注意到，子问题是一个背包问题，通过求解该问题，我们既可以得到未被restricted master problem考虑到的变量中最大的reduced cost rate，又能得到相应的系数列。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/pic12.png">
<h2 id="列生成算法经典例子-Cutting-Stock-Problem"><a href="#列生成算法经典例子-Cutting-Stock-Problem" class="headerlink" title="列生成算法经典例子 Cutting Stock Problem"></a>列生成算法经典例子 Cutting Stock Problem</h2><p>下面给出一个列生成法的经典例子，Cutting Stock Problem。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%872.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%873.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%874.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%875.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%876.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%877.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%878.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/column%20generation/%E5%9B%BE%E7%89%879.png">
<hr>
<p>Reference<br><a href="http://www.or.rwth-aachen.de/research/publications/primer.pdf" target="_blank" rel="external">http://www.or.rwth-aachen.de/research/publications/primer.pdf</a><br><a href="https://optimization.mccormick.northwestern.edu/index.php/Column_generation_algorithms" target="_blank" rel="external">https://optimization.mccormick.northwestern.edu/index.php/Column_generation_algorithms</a><br><a href="http://ocw.nctu.edu.tw/upload/classbfs121109080773803.pdf" target="_blank" rel="external">http://ocw.nctu.edu.tw/upload/classbfs121109080773803.pdf</a><br><a href="http://personal.vu.nl/l.stougie/Courses/ALP/BTonlyCh12345.pdf" target="_blank" rel="external">http://personal.vu.nl/l.stougie/Courses/ALP/BTonlyCh12345.pdf</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;最近在做混合整数规划相关的调研，主要是关于列生成算法 (Column Generation Algorithm)，搜了一圈，发现网路上并有很多相关的中文资料，难道这个算法太难了？还是普及度不够高？后来找到了一份相关的不错的英文讲义，从头看下来，也就知道了列生成算法大概是在干什么，但是其中原因还是不得要领。&lt;/p&gt;
&lt;p&gt;之后，我又认真复习了单纯形法，才明白了列生成算法的初衷和要领，希望本文能把它讲明白。&lt;/p&gt;
&lt;p&gt;（最近一直用图片的形式记录，是因为这些总结都是组会上的slides😅&lt;/p&gt;
    
    </summary>
    
    
      <category term="Column Generation Algorithm" scheme="https://xijunlee.github.io/tags/Column-Generation-Algorithm/"/>
    
      <category term="Operation Research" scheme="https://xijunlee.github.io/tags/Operation-Research/"/>
    
      <category term="Simplex Method" scheme="https://xijunlee.github.io/tags/Simplex-Method/"/>
    
  </entry>
  
  <entry>
    <title>极简总结两个矩阵分解</title>
    <link href="https://xijunlee.github.io/2017/10/01/%E6%9E%81%E7%AE%80%E6%80%BB%E7%BB%93%E4%B8%A4%E4%B8%AA%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3/"/>
    <id>https://xijunlee.github.io/2017/10/01/极简总结两个矩阵分解/</id>
    <published>2017-10-01T11:23:10.000Z</published>
    <updated>2017-10-01T11:26:55.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>矩阵分解在ml用的非常多，尤其是特征分解和奇异值分解。特征分解只能用于实对称矩阵，而奇异值分解适用于任何形式的矩阵。二者之间有紧密的联系，遂用以下三张图总结。</p>
<a id="more"></a>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MatrixFactorization/WechatIMG61.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MatrixFactorization/WechatIMG62.png">
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MatrixFactorization/WechatIMG60.png">
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;矩阵分解在ml用的非常多，尤其是特征分解和奇异值分解。特征分解只能用于实对称矩阵，而奇异值分解适用于任何形式的矩阵。二者之间有紧密的联系，遂用以下三张图总结。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Matrix Factorization" scheme="https://xijunlee.github.io/tags/Matrix-Factorization/"/>
    
  </entry>
  
  <entry>
    <title>通俗理解Dirichlet分布及其实践</title>
    <link href="https://xijunlee.github.io/2017/09/09/Dirichlet%E5%88%86%E5%B8%83%E4%B8%8EBeta%E5%88%86%E5%B8%83/"/>
    <id>https://xijunlee.github.io/2017/09/09/Dirichlet分布与Beta分布/</id>
    <published>2017-09-09T13:17:26.000Z</published>
    <updated>2017-11-04T04:08:06.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近项目中有一部分用到了Dirichlet分布及其采样，这个分布应该是本科阶段的概率论没有教过的吧（也可能是我上课走神😓）。于是，从头学习，整理如下。</p>
<h2 id="Beta分布"><a href="#Beta分布" class="headerlink" title="Beta分布"></a>Beta分布</h2><p>要说Dirichlet分布之前，先了解下其特殊情况Beta分布。一言以蔽之，Beta分布是二项分布的分布。二项分布中的参数theta在Beta分布中，也被认为是一个随机变量，因此Beta分布是用来描述这个随机变量theta的。下面用知乎上一个通俗的例子来说明（我觉得这个例子讲得相当好）。更严谨的叙述参看<a href="https://mqshen.gitbooks.io/prml/Chapter2/binary/beta_distribute.html" target="_blank" rel="external">PRML中相关章节</a>。</p>
<a id="more"></a>
<p>用一句话来说，beta分布可以看作一个概率的概率分布，当你不知道一个东西的具体概率是多少时，它可以给出了所有概率出现的可能性大小。</p>
<p>举一个简单的例子，熟悉棒球运动的都知道有一个指标就是棒球击球率(batting average)，就是用一个运动员击中的球数除以击球的总数，我们一般认为0.266是正常水平的击球率，而如果击球率高达0.3就被认为是非常优秀的。现在有一个棒球运动员，我们希望能够预测他在这一赛季中的棒球击球率是多少。你可能就会直接计算棒球击球率，用击中的数除以击球数，但是如果这个棒球运动员只打了一次，而且还命中了，那么他就击球率就是100%了，这显然是不合理的，因为根据棒球的历史信息，我们知道这个击球率应该是0.215到0.36之间才对啊。对于这个问题，我们可以用一个二项分布表示（一系列成功或失败），一个最好的方法来表示这些经验（在统计中称为先验信息）就是用beta分布，这表示在我们没有看到这个运动员打球之前，我们就有了一个大概的范围。beta分布的定义域是(0,1)这就跟概率的范围是一样的。接下来我们将这些先验信息转换为beta分布的参数，我们知道一个击球率应该是平均0.27左右，而他的范围是0.21到0.35，那么根据这个信息，我们可以取α=81,β=219。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/dirichlet/1.png">
<p>这是因为：</p>
<ol>
<li>beta分布的均值是$$\frac{\alpha}{\alpha+\beta}=\frac{81}{81+219}=0.27$$</li>
<li>从上图中，我们可以看到该分布主要落在(0.2,0.35)间，这就是从先验中得到的知识。</li>
</ol>
<p>在这个例子里，我们的x轴就表示各个击球率的取值，x对应的y值就是这个击球率所对应的概率。也就是说beta分布可以看作一个<strong>概率的概率分布</strong>。那么有了先验信息后，现在我们考虑一个运动员只打一次球，那么他现在的数据就是”1中;1击”。这时候我们就可以<strong>更新我们的分布</strong>了，让这个曲线<strong>做一些移动去适应我们的新信息</strong>。beta分布在数学上就给我们提供了这一性质，他与二项分布是<strong>共轭先验</strong>的（Conjugate_prior）。所谓共轭先验就是先验分布是beta分布，而后验分布同样是beta分布。结果很简单：</p>
<p>$$ \mbox{Beta}(\alpha_0+\mbox{hits}, \beta_0+\mbox{misses})$$<br>其中α0和β0是一开始的参数，在这里是81和219。所以在这一例子里，α增加了1(击中了一次)。β没有增加(没有漏球)。这就是我们的新的beta分布Beta(81+1,219)，我们跟原来的比较一下：</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/dirichlet/2.png">
<p>可以看到这个分布其实没多大变化，这是因为只打了1次球并不能说明什么问题。但是如果我们得到了更多的数据，假设一共打了300次，其中击中了100次，200次没击中，那么这一新分布就是：</p>
<p>$$\mbox{Beta}(81+100, 219+200)$$</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/dirichlet/3.png">
<p>注意到这个曲线变得更加尖，并且平移到了一个右边的位置，表示比平均水平要高。</p>
<p>一个有趣的事情是，根据这个新的beta分布，我们可以得出他的数学期望为：</p>
<p>$$\frac{\alpha}{\alpha+\beta}=\frac{81+100}{81+100+219+200}=.303$$</p>
<p>这一结果要比直接的估计要小：</p>
<p>$$\frac{100}{100+200}=.333$$</p>
<p>你可能已经意识到，我们事实上就是在这个运动员在击球之前可以理解为他已经成功了81次，失败了219次这样一个<strong>先验信息</strong>。</p>
<p>因此，对于一个我们不知道概率theta是什么的二项分布，而又有一些合理的猜测时，beta分布能很好的作为描述该概率theta的概率分布。</p>
<p><strong>结论1</strong>：二项分布与Beta分布是<strong>共轭先验</strong>的。即Beta分布乘上一个二项分布的似然函数后，得到的后验分布仍然是一个Beta分布。</p>
<p>关于其的证明，可以参考reference中的<a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">链接</a>。</p>
<h2 id="Dirichlet分布"><a href="#Dirichlet分布" class="headerlink" title="Dirichlet分布"></a>Dirichlet分布</h2><p>先给出一个结论:</p>
<p><strong>结论2</strong>：Dirichlet分布就是由2种结果bernoulli trial导出的Beta分布外推到k种的一般情形。</p>
<p>详细一点地说，二项分布的分布是Beta分布，而二项分布的推广形式多项分布的分布就是Dirichlet分布。</p>
<p><strong>结论3</strong>：Dirichlet分布与多项分布是共轭先验的。即Dirichlet分布乘上一个多项分布的似然函数后，得到的后验分布仍然是一个Dirichlet分布。</p>
<p>严谨的叙述参考<a href="https://mqshen.gitbooks.io/prml/Chapter2/multinomial/dirichlet_distribute.html" target="_blank" rel="external">PRML相关章节</a>。</p>
<h2 id="实践：Dirichlet分布的估计"><a href="#实践：Dirichlet分布的估计" class="headerlink" title="实践：Dirichlet分布的估计"></a>实践：Dirichlet分布的估计</h2><p>即在给定多项分布中概率向量p的一些样本后，如何估计Dirichlet分布中的参数alpha。关于这个问题，Microsoft Research的<a href="https://tminka.github.io" target="_blank" rel="external">Tom Minka</a>给出了两种解决办法。无外乎都是极大似然法，两种方法的差异在于第一种方法是采用拟牛顿法来迭代求最大似然估计，而第二种方法则是稍微曲线救国了一下，先定义两个与alpha相关的值，通过先求这两个值从而求得alpha的最大似然估计，其中采用了EM的思想。具体细节可以参考其<a href="https://tminka.github.io/papers/dirichlet/minka-dirichlet.pdf" target="_blank" rel="external">论文</a>。</p>
<p>Minka大神同时也实现了其提出的两种方法的<a href="https://github.com/tminka/fastfit" target="_blank" rel="external">Matlab源码</a>，而<a href="http://www.ericsuh.com" target="_blank" rel="external">Eric J. Suh</a>将这些算法又用Python实现了一遍，做成了一个独立的<a href="https://github.com/ericsuh/dirichlet" target="_blank" rel="external">Python Package</a>。我项目中就是直接用的这个，跪谢两位大神Orz!</p>
<hr>
<p>Reference<br><a href="https://github.com/tminka/fastfit" target="_blank" rel="external">https://github.com/tminka/fastfit</a><br><a href="https://tminka.github.io/papers/dirichlet/minka-dirichlet.pdf" target="_blank" rel="external">https://tminka.github.io/papers/dirichlet/minka-dirichlet.pdf</a><br><a href="https://www.zhihu.com/question/30269898" target="_blank" rel="external">https://www.zhihu.com/question/30269898</a><br><a href="https://mqshen.gitbooks.io/prml/Chapter2/multinomial/dirichlet_distribute.html" target="_blank" rel="external">https://mqshen.gitbooks.io/prml/Chapter2/multinomial/dirichlet_distribute.html</a><br><a href="https://mqshen.gitbooks.io/prml/Chapter2/binary/beta_distribute.html" target="_blank" rel="external">https://mqshen.gitbooks.io/prml/Chapter2/binary/beta_distribute.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;最近项目中有一部分用到了Dirichlet分布及其采样，这个分布应该是本科阶段的概率论没有教过的吧（也可能是我上课走神😓）。于是，从头学习，整理如下。&lt;/p&gt;
&lt;h2 id=&quot;Beta分布&quot;&gt;&lt;a href=&quot;#Beta分布&quot; class=&quot;headerlink&quot; title=&quot;Beta分布&quot;&gt;&lt;/a&gt;Beta分布&lt;/h2&gt;&lt;p&gt;要说Dirichlet分布之前，先了解下其特殊情况Beta分布。一言以蔽之，Beta分布是二项分布的分布。二项分布中的参数theta在Beta分布中，也被认为是一个随机变量，因此Beta分布是用来描述这个随机变量theta的。下面用知乎上一个通俗的例子来说明（我觉得这个例子讲得相当好）。更严谨的叙述参看&lt;a href=&quot;https://mqshen.gitbooks.io/prml/Chapter2/binary/beta_distribute.html&quot;&gt;PRML中相关章节&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Probability Theory" scheme="https://xijunlee.github.io/tags/Probability-Theory/"/>
    
  </entry>
  
  <entry>
    <title>多目标优化方法之MOEA-D</title>
    <link href="https://xijunlee.github.io/2017/08/29/%E5%A4%9A%E7%9B%AE%E6%A0%87%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E4%B9%8BMOEA-D/"/>
    <id>https://xijunlee.github.io/2017/08/29/多目标优化方法之MOEA-D/</id>
    <published>2017-08-29T14:57:35.000Z</published>
    <updated>2017-08-30T13:55:46.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>最近项目有一部分是在做多目标优化，调研了一些论文，总结一下其中目前state-of-the-art的solution —— <a href="http://ieeexplore.ieee.org/document/4358754/" target="_blank" rel="external">MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition</a>，这篇paper出自两位华人之手Qingfu Zhang和Hui Li。</p>
<hr>
<p>下面用我组会slides上的两张图简单介绍问题和这篇paper提出的解决思路。</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MOEAD/MOEAD%20summary.png">
<a id="more"></a>
<p>对于多目标优化问题F，MOEA/D的解决思路非常之简单，就是通过权重向量lamda将多个单优化目标组合到一起，将该问题转为单目标优化问题。乍一看，非常简单呢。但是其通过权重向量lambda为遗传算法中的候选解定义了邻居的概念，通过与邻居共同进化的思想，寻找pareto最优。但我觉得其缺点还是需要人为地去设置权重向量们，权重向量设置的好坏将决定最后解的质量。以下是具体算法的一个简单流程图：</p>
<img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MOEAD/MOEAD%20flow%20chart.png">
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;最近项目有一部分是在做多目标优化，调研了一些论文，总结一下其中目前state-of-the-art的solution —— &lt;a href=&quot;http://ieeexplore.ieee.org/document/4358754/&quot;&gt;MOEA/D: A Multiobjective Evolutionary Algorithm Based on Decomposition&lt;/a&gt;，这篇paper出自两位华人之手Qingfu Zhang和Hui Li。&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;下面用我组会slides上的两张图简单介绍问题和这篇paper提出的解决思路。&lt;/p&gt;
&lt;img src=&quot;http://xijun-album.oss-cn-hangzhou.aliyuncs.com/MOEAD/MOEAD%20summary.png&quot;&gt;
    
    </summary>
    
    
      <category term="Multiobjective Optimization" scheme="https://xijunlee.github.io/tags/Multiobjective-Optimization/"/>
    
  </entry>
  
  <entry>
    <title>机器学习中常见激活函数总结</title>
    <link href="https://xijunlee.github.io/2017/06/25/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E5%B8%B8%E8%A7%81%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
    <id>https://xijunlee.github.io/2017/06/25/机器学习中常见激活函数总结/</id>
    <published>2017-06-25T13:58:32.000Z</published>
    <updated>2017-11-06T13:04:57.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>机器学习中很多场合都用到了激活函数 (Activation Function)，但激活函数主要是在神经网络中提出来的，很多场合下其实就是函数。这篇总结一下常见的激活函数以及它们的优缺点。</p>
<h2 id="激活函数起源与性质"><a href="#激活函数起源与性质" class="headerlink" title="激活函数起源与性质"></a>激活函数起源与性质</h2><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/ActivationFunction/pic0.png">
<a id="more"></a>
<p>一个人工神经元就是对生物神经元的数学建模。人工神经元就是用一个数学模型简单模拟神经细胞。神经细胞有多个树突和一个伸长的轴突。一个神经元的轴突连接到其他神经元的树突，并向其传导神经脉冲。神经元会根据来自它的若干树突的信号决定是否从其轴突向其他神经元发出神经脉冲。而激活函数就是这个神经元的数学模型，它决定是否将经过变换后的上一层输入信号传递给下一层神经元。激活函数的性质有：</p>
<ol>
<li>非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。</li>
<li>可微性： 当优化方法是基于梯度的时候，这个性质是必须的。</li>
<li>单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。</li>
<li>输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate.</li>
</ol>
<p>常见的激活函数总结如下图：<br><img src="http://xijun-album.oss-cn-hangzhou.aliyuncs.com/ActivationFunction/pic1.png"></p>
<h2 id="Sigmoid系激活函数"><a href="#Sigmoid系激活函数" class="headerlink" title="Sigmoid系激活函数"></a>Sigmoid系激活函数</h2><p>Sigmoid系激活函数是一大类函数，其函数形状是S型的，因此而得名。代表有Sigmoid函数和tanh函数。</p>
<p>$$sigmoid: f(x)=\frac{1}{1+e^{-x}}$$<br>$$tanh: f(x)=\frac{2}{1+e^{-2x}}-1=2Sigmoid(2x)-1$$</p>
<p>Sigmoid系是之前使用的最多的激活函数，它在物理意义上最为接近生物神经元，能够把输入的连续实值“压缩”到[0,1]或者[-1,1]。此外，[0,1]的输出还可以被表示作概率，或用于输入的归一化，代表性的如Sigmoid交叉熵损失函数。此外，Sigmoid函数还被用在逻辑斯蒂回归中，在那里，它被叫做逻辑斯蒂函数 (Logistic Function)。所以我怎么老是看到这个函数的身影，原来它在不同场景中穿的马甲不同。既然提到了回归，那么就再提一下逻辑斯蒂回归的一般形式Softmax回归中的Softmax函数。逻辑斯蒂回归是种类数k=2时的Softmax回归,有关二者更多详情请戳<a href="http://www.cnblogs.com/maybe2030/p/5678387.html" target="_blank" rel="external">这里</a>和<a href="http://ufldl.stanford.edu/wiki/index.php/Softmax回归" target="_blank" rel="external">这里</a>。</p>
<p>回到Sigmoid系函数。近年来，用它的人越来越少了。主要是因为它的一些缺点：<br>1.饱和性：从sigmoid函数图像中可以看到，其两侧增长十分缓慢，即越靠近两侧导数趋近于0。那么这在神经网络中训练的会带来梯度弥散 (Gradient Disperse) 的问题，使得训练一个神经网络十分缓慢，或者根本无法收敛。具体来说，在训练神经网络的反向传播算法中，需要计算梯度$\nabla=\sigma’\delta x$。其中$\sigma’$是sigmoid函数的导数。每经过一个sigmoid神经元，梯度就要乘上一个$\sigma’$。从下图可以看到，sigmoid函数导数的最大值是0.25。那么连续的乘以sigmoid的导数，会导致梯度越来越小。一般来说， sigmoid 网络在5层之内就会产生梯度消失现象这就是梯度弥散问题。这对于深层网络的训练是很大的问题，因此在如今大火的DNN中，sigmoid遭到抛弃。<br>2.sigmoid函数的输出均大于0：这使得输出不是0均值，这称为偏移现象，这会导致后一层的神经元将得到上一层输出的非0均值的信号作为输入。（关于这一点，我不是很理解）</p>
<p>tanh也是一种非常常见的激活函数。与sigmoid相比，它的输出均值是0，使得其收敛速度要比sigmoid快，减少迭代次数。然而，从途中可以看出，tanh一样具有软饱和性，从而造成梯度弥散。</p>
<h2 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h2><p>最近几年卷积神经网络中，激活函数往往不选择sigmoid或tanh函数，而是选择relu函数。Relu函数的定义是：</p>
<p>$$ReLU: f(x)=\max(0,x)$$</p>
<p>Relu函数作为激活函数，有下面几大优势：</p>
<p>1.速度快：和sigmoid函数需要计算指数和倒数相比，relu函数其实就是一个max(0,x)，计算代价小很多。<br>2.减轻梯度消失问题：relu函数在大于零的一侧其导数大于零，不会导致梯度变小。当然，激活函数仅仅是导致梯度减小的一个因素，但无论如何在这方面relu的表现强于sigmoid。使用relu激活函数可以让你训练更深的网络。</p>
<p>然而，随着训练的推进，部分输入会落入x&lt;0的区域，其梯度等于0，导致对应权重无法更新。这种现象被称为“神经元死亡”。与sigmoid类似，ReLU的输出均值也大于0，偏移现象和 神经元死亡会共同影响网络的收敛性。对此，相应的改进有Leaky-ReLU，ELU，见<a href="http://blog.csdn.net/u014595019/article/details/52562159" target="_blank" rel="external">参考链接</a>。</p>
<h2 id="Maxout"><a href="#Maxout" class="headerlink" title="Maxout"></a>Maxout</h2><p>$$Maxout: f(x)=\max(w^T_1x+b_1,w^T_2x+b_2,⋯,w^T_2x+b_2)$$</p>
<p>Maxout出现在ICML2013上，作者Goodfellow将maxout和dropout结合后，号称在MNIST, CIFAR-10, CIFAR-100, SVHN这4个数据上都取得了start-of-art的识别率。可以注意到，ReLU 和 Leaky ReLU 都是它的一个变形。这个激活函数有点大一统的感觉，因为maxout网络能够近似任意连续函数，且当w2,b2,…,wn,bn为0时，退化为ReLU。Maxout能够缓解梯度弥散，同时又规避了ReLU神经元死亡的缺点，但增加了参数和计算量。</p>
<hr>
<p>Reference<br><a href="http://blog.csdn.net/cyh_24/article/details/50593400" target="_blank" rel="external">http://blog.csdn.net/cyh_24/article/details/50593400</a><br><a href="http://blog.csdn.net/u014595019/article/details/52562159" target="_blank" rel="external">http://blog.csdn.net/u014595019/article/details/52562159</a><br><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a><br><a href="http://ufldl.stanford.edu/wiki/index.php/Softmax回归" target="_blank" rel="external">http://ufldl.stanford.edu/wiki/index.php/Softmax回归</a><br><a href="http://www.cnblogs.com/maybe2030/p/5678387.html" target="_blank" rel="external">http://www.cnblogs.com/maybe2030/p/5678387.html</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;机器学习中很多场合都用到了激活函数 (Activation Function)，但激活函数主要是在神经网络中提出来的，很多场合下其实就是函数。这篇总结一下常见的激活函数以及它们的优缺点。&lt;/p&gt;
&lt;h2 id=&quot;激活函数起源与性质&quot;&gt;&lt;a href=&quot;#激活函数起源与性质&quot; class=&quot;headerlink&quot; title=&quot;激活函数起源与性质&quot;&gt;&lt;/a&gt;激活函数起源与性质&lt;/h2&gt;&lt;img src=&quot;http://xijun-album.oss-cn-hangzhou.aliyuncs.com/ActivationFunction/pic0.png&quot;&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="Activation Function" scheme="https://xijunlee.github.io/tags/Activation-Function/"/>
    
  </entry>
  
  <entry>
    <title>卷积神经网络实践之MNIST手写数字体识别</title>
    <link href="https://xijunlee.github.io/2017/06/23/CNN%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    <id>https://xijunlee.github.io/2017/06/23/CNN实践之手写数字体识别/</id>
    <published>2017-06-23T07:46:26.000Z</published>
    <updated>2017-11-06T13:08:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>之前一直都是只看理论，没有动手，现在终于把理论付诸实践了。话不多说，这篇博文记录一下我在kaggle上实践过程。</p>
<h2 id="任务描述"><a href="#任务描述" class="headerlink" title="任务描述"></a>任务描述</h2><p>MNIST (“Modified National Institute of Standards and Technology”) is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.</p>
<a id="more"></a>
<p>In this competition, your goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. We’ve curated a set of tutorial-style kernels which cover everything from regression to neural networks. We encourage you to experiment with different algorithms to learn first-hand what works well and how techniques compare.</p>
<h2 id="解决思路"><a href="#解决思路" class="headerlink" title="解决思路"></a>解决思路</h2><p>其实这就是个多分类问题，所以常见的分类器都能解决上述问题。kaggle上的大神们也给出了卷积神经网络以外的思路：比如PCA+SVM。类似的还有很多，比如可以PCA+RandomForest, PCA+Xgboost, PCA+blahblahblah … </p>
<p>但最牛逼的解决方案还是CNN，我自己写了一个PCA+SVM的kernel，得分比CNN低很多。当然，如果做好feature engeering和好好调参的话，应该还是能取得不错的效果。但是肯定不如CNN厉害，不然现在CNN为什么会在图像识别领域大火呢？</p>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>要我手写一个CNN是不可能的，既太花时间，我也暂时没那个本事。所以，我就用了Keras+ TensorFlow来实现一个卷积神经网络。</p>
<p>官网介绍：<a href="https://keras.io" target="_blank" rel="external">Keras</a> is a high-level neural networks API, written in Python and capable of running on top of either TensorFlow, CNTK or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.</p>
<p>官网介绍：<a href="https://www.tensorflow.org" target="_blank" rel="external">TensorFlow</a> is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) that flow between them. This flexible architecture lets you deploy computation to one or more CPUs or GPUs in a desktop, server, or mobile device without rewriting code. TensorFlow also includes TensorBoard, a data visualization toolkit.</p>
<p>Keras是一个高层次抽象的卷积神经网络python库，但它本身是不会做矩阵计算等任务的，需要以TensorFlow作为后端来进行复杂的底层计算。在TensorFlow的基础上，Keras能非常方便地调用各种api来完成卷积神经网络的搭建。下面，简单地记录一下安装Kera+TensorFlow的过程。</p>
<h3 id="TensorFlow安装-on-Ubuntu-without-GPU-support"><a href="#TensorFlow安装-on-Ubuntu-without-GPU-support" class="headerlink" title="TensorFlow安装 (on Ubuntu without GPU support)"></a>TensorFlow安装 (on Ubuntu without GPU support)</h3><p>这里，我用的系统是Ubuntu 14.04.1 LTS (GNU/Linux 3.13.0-32-generic x86_64), python是3.4.3，GCC是4.8.4，所以下面的安装就是基于以上环境。建议采用pip的方式安装TensorFlow，下面keras也一样，简直是太方便了。因为实验室服务器没有GPU，所以我就只能安装CPU版本的TensorFlow了。</p>
<p>1.创建虚拟环境</p>
<p>为不同的项目创建独立的python虚拟环境是一个良好的习惯。利用不同的python虚拟环境来做不同的事，以免不同项目的环境相互影响了。以下两行代码分别是安装python 2.7和python 3.4+虚拟环境的命令，任君选择。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sudo apt-get install python-pip python-dev python-virtualenv    <span class="comment"># python 2.7</span></div><div class="line">$ sudo apt-get install python3-pip python3-dev python3-virtualenv <span class="comment"># python 3.4+</span></div></pre></td></tr></table></figure></p>
<p>2.启动虚拟环境<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ virtualenv --system-site-packages ~/tensorflow</div><div class="line">$ <span class="built_in">source</span> ~/tensorflow/bin/activate</div></pre></td></tr></table></figure></p>
<p>3.安装TensorFlow<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Python 2</span></div><div class="line">$ sudo pip install --upgrade tensorflow</div><div class="line"> </div><div class="line"><span class="comment"># Python 3</span></div><div class="line">$ sudo pip3 install --upgrade tensorflow</div></pre></td></tr></table></figure></p>
<h3 id="Keras安装"><a href="#Keras安装" class="headerlink" title="Keras安装"></a>Keras安装</h3><p>1.安装所需要的包和keras</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ pip install numpy scipy</div><div class="line">$ pip install scikit-learn</div><div class="line">$ pip install pillow</div><div class="line">$ pip install h5py</div><div class="line">$ pip install keras</div></pre></td></tr></table></figure>
<p>2.编辑配置文件keras.json，将keras的计算后端设置为TensorFlow：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim ~/.keras/keras.json</div></pre></td></tr></table></figure></p>
<p>确保上述文件内容为:<br><figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&#123;</div><div class="line">    <span class="attr">"image_dim_ordering"</span>: <span class="string">"tf"</span>, </div><div class="line">    <span class="attr">"epsilon"</span>: <span class="number">1e-07</span>, </div><div class="line">    <span class="attr">"floatx"</span>: <span class="string">"float32"</span>, </div><div class="line">    <span class="attr">"backend"</span>: <span class="string">"tensorflow"</span></div><div class="line">&#125;</div></pre></td></tr></table></figure></p>
<p>image_dim_ordering：TensorFlow是使用NumPy数组 (height, width, depth)。如果你用Theano，数组表示方法(depth, height, width)。</p>
<p>打开python，输入以下代码<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> Keras</div></pre></td></tr></table></figure></p>
<p>如果出现以下代码，表示基于TensorFlow的Keras安装成功！！<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> keras</div><div class="line">Using TensorFlow backend.</div></pre></td></tr></table></figure></p>
<h2 id="实现与结果"><a href="#实现与结果" class="headerlink" title="实现与结果"></a>实现与结果</h2><p>这里给出我利用keras实现的一个多层卷积神经网络，源码戳<a href="https://github.com/xijunlee/kaggle-solution/blob/master/DigitRecognizer/DigitRec_CNN.py" target="_blank" rel="external">这里</a>，该网络的结构如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line">_________________________________________________________________</div><div class="line">Layer (type)                 Output Shape              Param #</div><div class="line">=================================================================</div><div class="line">conv2d_1 (Conv2D)            (None, 28, 28, 32)        320</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_2 (Conv2D)            (None, 28, 28, 32)        9248</div><div class="line">_________________________________________________________________</div><div class="line">max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_3 (Conv2D)            (None, 14, 14, 64)        18496</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_4 (Conv2D)            (None, 14, 14, 64)        36928</div><div class="line">_________________________________________________________________</div><div class="line">max_pooling2d_2 (MaxPooling2 (None, 7, 7, 64)          0</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_5 (Conv2D)            (None, 7, 7, 128)         73856</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_6 (Conv2D)            (None, 7, 7, 128)         147584</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_7 (Conv2D)            (None, 7, 7, 128)         147584</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_8 (Conv2D)            (None, 7, 7, 128)         147584</div><div class="line">_________________________________________________________________</div><div class="line">max_pooling2d_3 (MaxPooling2 (None, 3, 3, 128)         0</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_9 (Conv2D)            (None, 3, 3, 256)         295168</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_10 (Conv2D)           (None, 3, 3, 256)         590080</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_11 (Conv2D)           (None, 3, 3, 256)         590080</div><div class="line">_________________________________________________________________</div><div class="line">conv2d_12 (Conv2D)           (None, 3, 3, 256)         590080</div><div class="line">_________________________________________________________________</div><div class="line">max_pooling2d_4 (MaxPooling2 (None, 1, 1, 256)         0</div><div class="line">_________________________________________________________________</div><div class="line">flatten_1 (Flatten)          (None, 256)               0</div><div class="line">_________________________________________________________________</div><div class="line">dropout_1 (Dropout)          (None, 256)               0</div><div class="line">_________________________________________________________________</div><div class="line">dense_1 (Dense)              (None, 128)               32896</div><div class="line">_________________________________________________________________</div><div class="line">dense_2 (Dense)              (None, 10)                1290</div><div class="line">=================================================================</div><div class="line">Total params: 2,681,194</div><div class="line">Trainable params: 2,681,194</div><div class="line">Non-trainable params: 0</div></pre></td></tr></table></figure></p>
<p>我是故意把这个搞的比较复杂，最后kaggle上LB得分为0.98971, rank 652/1924，很多人做到了score为1的地步……</p>
<p>我前两个卷积层为例说明一下卷积神经网络中参数个数的理解。</p>
<p>首先，我的输入是一个28x28的灰度图，那么输入层是一个28x28x1的结构，那个1是该层的深度。</p>
<p>conv2d_1：第一个卷积层，总共设置了32个filters(滤波器，也叫卷积核，这家伙也是有多个马甲的)。卷积核大小设置为3x3。那么经过32个filter的卷积后，输入层的图像变成了32个28x28的卷积层。（注意filter的数量等于该层的深度）。那么输入层到第一层卷积层的参数数量是怎么计算的呢？因为每个卷积核有3x3个参数，然后每个卷积核还要带一个bias参数，所以一个卷积核有3x3x1+1=10个参数。conv2d_1总共有32个卷积核，并且每个卷积核的深度是1，所以总共有(3x3x1+1)x32=320个参数。这个1看起来乘地没必要，但是它代表了每个卷积核的深度。卷积核的深度与上一层的输出的深度相等。</p>
<p>con2d_2：第二个卷积层，同样设置了32个filters，卷积核大小同样设置为3x3。conv2d_1的输出是28x28x32的结构，即深度为32的28x28的图像。自然，con2d_2的filter的深度也为32。经过con2d_2的32个filter后，卷积得到依旧是28x28x32的输出。参数的数量怎么计算呢？对于con2d_2的一个卷积核来说，其深度是32，每一层的大小是3x3，再外加一个bias参数，那么一个卷积核的参数数量便是3x3x32+1=289个。然后总共有32个卷积核，所以总共的参数数量是(3x3x32+1)x32=9248。</p>
<p>可能con2d_2的参数数量说明的让人头晕，更详细和生动的说明可以参见<a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">博文</a>，里面有动图说明了对于深度大于1的卷积核的卷积操作。</p>
<h3 id="关于引入冲量"><a href="#关于引入冲量" class="headerlink" title="关于引入冲量"></a>关于引入冲量</h3><p>本小节内容转载自 @辛淼 CaffeCN社区（caffecn.cn）</p>
<p>随机梯度下降（SGD）是按batch来进行更新，通常来说下降速度比较快，但却容易造成另一个问题，就是更新过程不稳定，容易出现震荡。</p>
<p>引入momentum的idea是很直接的，就是在更新下降方向的时候不仅要考虑到当前的方向，也要考虑到上一次的更新方向，两者加权，某些情况下可以避免震荡。冲量，就是<strong>上一次更新方向所占的权值</strong>。</p>
<p>一个小的trick是，当刚开始训练的时候，把冲量设小，或者直接就置为0，然后慢慢增大冲量，有时候效果比较好。</p>
<h3 id="PCA-SVM的实现"><a href="#PCA-SVM的实现" class="headerlink" title="PCA+SVM的实现"></a>PCA+SVM的实现</h3><p>之后，我又尝试了PCA+SVM的模型，源码戳<a href="https://github.com/xijunlee/kaggle-solution/blob/master/DigitRecognizer/DigitRec_SVM.py" target="_blank" rel="external">这里</a>。因为没有专门去调参数，所以LB得分很低，才0.49743…… 不过，我想就算找到了最优参数，其效果也应该比不上卷积神经网络。</p>
<h2 id="DNN零基础入门教程推荐"><a href="#DNN零基础入门教程推荐" class="headerlink" title="DNN零基础入门教程推荐"></a>DNN零基础入门教程推荐</h2><p>最后，推荐一组深度学习的入门教程，我认为还不错。</p>
<p><a href="https://www.zybuluo.com/hanbingtao/note/433855" target="_blank" rel="external">零基础入门深度学习(1) - 感知器</a><br><a href="https://www.zybuluo.com/hanbingtao/note/448086" target="_blank" rel="external">零基础入门深度学习(2) - 线性单元和梯度下降</a><br><a href="https://www.zybuluo.com/hanbingtao/note/476663" target="_blank" rel="external">零基础入门深度学习(3) - 神经网络和反向传播算法</a><br><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">零基础入门深度学习(4) - 卷积神经网络</a><br><a href="https://zybuluo.com/hanbingtao/note/541458" target="_blank" rel="external">零基础入门深度学习(5) - 循环神经网络</a><br><a href="https://zybuluo.com/hanbingtao/note/581764" target="_blank" rel="external">零基础入门深度学习(6) - 长短时记忆网络(LSTM)</a><br><a href="https://zybuluo.com/hanbingtao/note/626300" target="_blank" rel="external">零基础入门深度学习(7) - 递归神经网络</a></p>
<hr>
<p>Reference<br><a href="https://www.kaggle.com/poonaml/deep-neural-network-keras-way" target="_blank" rel="external">https://www.kaggle.com/poonaml/deep-neural-network-keras-way</a><br><a href="http://blog.topspeedsnail.com/archives/10427" target="_blank" rel="external">http://blog.topspeedsnail.com/archives/10427</a><br><a href="https://keras.io" target="_blank" rel="external">https://keras.io</a><br><a href="https://www.kaggle.com/somshubramajumdar/deep-convolutional-network-using-keras" target="_blank" rel="external">https://www.kaggle.com/somshubramajumdar/deep-convolutional-network-using-keras</a><br><a href="https://www.zybuluo.com/hanbingtao/note/485480" target="_blank" rel="external">https://www.zybuluo.com/hanbingtao/note/485480</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;之前一直都是只看理论，没有动手，现在终于把理论付诸实践了。话不多说，这篇博文记录一下我在kaggle上实践过程。&lt;/p&gt;
&lt;h2 id=&quot;任务描述&quot;&gt;&lt;a href=&quot;#任务描述&quot; class=&quot;headerlink&quot; title=&quot;任务描述&quot;&gt;&lt;/a&gt;任务描述&lt;/h2&gt;&lt;p&gt;MNIST (“Modified National Institute of Standards and Technology”) is the de facto “hello world” dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. As new machine learning techniques emerge, MNIST remains a reliable resource for researchers and learners alike.&lt;/p&gt;
    
    </summary>
    
    
      <category term="Machine Learning" scheme="https://xijunlee.github.io/tags/Machine-Learning/"/>
    
      <category term="CNN" scheme="https://xijunlee.github.io/tags/CNN/"/>
    
  </entry>
  
</feed>
